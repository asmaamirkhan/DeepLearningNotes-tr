# ğŸš™ Ã–ÄŸrenme AktarmasÄ± \(Transfer Learning\)

KÄ±saca: Bir gÃ¶revden Ã¶ÄŸrenmek ve Ã¶ÄŸrenilenleri baÅŸka farklÄ± gÃ¶revler iÃ§in kullanmak ğŸ›°ğŸš™

## â“ Ã–ÄŸrenme AktarmasÄ± Nedir?

* ğŸ•µï¸â€â™€ï¸ Ã–ÄŸrenme AktarmasÄ±, bir gÃ¶rev iÃ§in eÄŸitilmiÅŸ bir modelin, ilgili ikinci bir gÃ¶revde yeniden kullanÄ±ldÄ±ÄŸÄ± bir makine Ã¶ÄŸrenme tekniÄŸidir. 
* ğŸŒŸ Ek olarak, ikinci gÃ¶revi modellerken hÄ±zlÄ± ilerleme veya geliÅŸmiÅŸ performans saÄŸlayan bir optimizasyon yÃ¶ntemidir.. 
* ğŸ¤¸â€â™€ï¸ Ã–ÄŸrenme AktarmasÄ±, yalnÄ±zca ilk gÃ¶revden Ã¶ÄŸrenilen model Ã¶zellikleri genel ise, derin Ã¶ÄŸrenmede iÅŸe yarar.

> Uzun lafÄ±n kÄ±sasÄ±: Bir sinir aÄŸÄ±nÄ± sÄ±fÄ±rdan eÄŸitmek yerine, bir baÅŸkasÄ±nÄ±n zaten haftalarca bÃ¼yÃ¼k bir veri setinde eÄŸittiÄŸi aÃ§Ä±k kaynaklÄ± bir model indirebiliriz ve bu parametreleri modelimizi elimizdeki biraz daha kÃ¼Ã§Ã¼k veri setiyle eÄŸitmek iÃ§in baÅŸlangÄ±Ã§ noktasÄ± olarak kullanabiliriz. âœ¨

## ğŸ’« Geleneksel ML - Ã–ÄŸrenme AktarmasÄ±

![](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/c9ee03241414e86f59a83b656e48738150bfa1bb/.gitbook/assets/mlvstl.png)

## ğŸ™„ Problem

Bir sinir aÄŸÄ±ndaki katmanlar bazen benzer aÄŸÄ±rlÄ±klara sahip olabilir ve birbirlerini **aÅŸÄ±rÄ± Ã¶ÄŸrenme** neden olacak ÅŸekilde etkileyebilirler. BÃ¼yÃ¼k karmaÅŸÄ±k bir model ile bu bir risktir. Yani eÄŸer _Dense_ katmanlarÄ±nÄ± hayal ettiÄŸimizde, biraz buna benzeyebilir.

KomÅŸularla benzer aÄŸÄ±rlÄ±klarÄ± olan bazÄ± nÃ¶ronlarÄ± kaldÄ±rabiliriz, bÃ¶ylece over fitting kaldÄ±rÄ±lÄ±r..

### ğŸ”ƒ KarÅŸÄ±laÅŸtÄ±rma

![](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/c9ee03241414e86f59a83b656e48738150bfa1bb/.gitbook/assets/nnwithoutdropout.JPG) ![](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/c9ee03241414e86f59a83b656e48738150bfa1bb/.gitbook/assets/nnwithdropout.JPG)

> ğŸ¤¸â€â™€ï¸ Dropout iÅŸleminden Ã¶nce ve sonra bir NN

![](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/c9ee03241414e86f59a83b656e48738150bfa1bb/.gitbook/assets/accuracywithoutdropout.JPG) ![](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/c9ee03241414e86f59a83b656e48738150bfa1bb/.gitbook/assets/accuracywithdropout.JPG)

> âœ¨ SÃ¶nÃ¼mleme \(dropout\) iÅŸleminden Ã¶nce ve sonra accuracy durumu

## ğŸ¤” Ne zaman kullanÄ±lmasÄ± pratiktir?

Kendisinden aktardÄ±ÄŸÄ±mÄ±z problem \(kaynak\) iÃ§in Ã§ok fazla veriye sahip olduÄŸumuzda ve aktardÄ±ÄŸÄ±mÄ±z problem \(hedef\) iÃ§in genellikle nispeten daha az veri olduÄŸunda pratiktir ğŸ•µï¸â€

**Daha doÄŸrusu:**

`gÃ¶rev A` ve `gÃ¶rev B` iÃ§in, **A'dan B'ye** aktarmak aÅŸaÄŸÄ±daki durumlarda makul olabilir:

* ğŸš© GÃ¶rev A ve gÃ¶rev B aynÄ± Ã§Ä±kÄ±ÅŸa x sahipler ise
* â­ `gÃ¶rev A` iÃ§in `gÃ¶rev B`'ye gÃ¶re daha Ã§ok veriye sahip isek  
* ğŸ” `gÃ¶rev A`'nÄ±n DÃ¼ÅŸÃ¼k seviye Ã¶zellikleri `task B`'yi Ã¶ÄŸrenmek iÃ§in faydalÄ± ise 

## ğŸŒ YazÄ±nÄ±n AslÄ±

* [Burada ğŸ¾](https://dl.asmaamir.com/5-dlstrategies/b-transferlearning)

## ğŸ§ Referanslar

* [More about transfer learning in Tensorflow](https://www.tensorflow.org/tutorials/images/transfer_learning)
* [Understanding Dropout](https://www.youtube.com/watch?v=ARq74QuavAo)


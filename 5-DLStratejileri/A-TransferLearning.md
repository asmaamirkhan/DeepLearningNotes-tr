# ğŸš™ Transfer Learning
KÄ±saca: Bir gÃ¶revden Ã¶ÄŸrenmek ve Ã¶ÄŸrenilenleri baÅŸka farklÄ± gÃ¶revler iÃ§in kullanmak ğŸ›°ğŸš™

## â“ Transfer Learning Nedir?
- ğŸ•µï¸â€â™€ï¸ Transfer Learning, bir gÃ¶rev iÃ§in eÄŸitilmiÅŸ bir modelin, ilgili ikinci bir gÃ¶revde yeniden kullanÄ±ldÄ±ÄŸÄ± bir makine Ã¶ÄŸrenme tekniÄŸidir. 
- ğŸŒŸ Ek olarak, ikinci gÃ¶revi modellerken hÄ±zlÄ± ilerleme veya geliÅŸmiÅŸ performans saÄŸlayan bir optimizasyon yÃ¶ntemidir.. 
- ğŸ¤¸â€â™€ï¸ Transfer Learning, yalnÄ±zca ilk gÃ¶revden Ã¶ÄŸrenilen model Ã¶zellikleri genel ise, derin Ã¶ÄŸrenmede iÅŸe yarar.

> Uzun lafÄ±n kÄ±sasÄ±: Bir sinir aÄŸÄ±nÄ± sÄ±fÄ±rdan eÄŸitmek yerine, bir baÅŸkasÄ±nÄ±n zaten haftalarca bÃ¼yÃ¼k bir veri setinde eÄŸittiÄŸi aÃ§Ä±k kaynaklÄ± bir model indirebiliriz ve bu parametreleri modelimizi elimizdeki biraz daha kÃ¼Ã§Ã¼k veri setiyle eÄŸitmek iÃ§in baÅŸlangÄ±Ã§ noktasÄ± olarak kullanabiliriz. âœ¨

## ğŸ’« Geleneksel ML - Transfer Learning

<img src="../res/MLvsTL.png" width="450"  />


## ğŸ™„ Problem
Bir sinir aÄŸÄ±ndaki katmanlar bazen benzer aÄŸÄ±rlÄ±klara sahip olabilir ve birbirlerini **over-fitting** neden olacak ÅŸekilde etkileyebilirler. BÃ¼yÃ¼k karmaÅŸÄ±k bir model ile bu bir risktir. Yani eÄŸer _Dense_ katmanlarÄ±nÄ± hayal ettiÄŸimizde, biraz buna benzeyebilir.

KomÅŸularla benzer aÄŸÄ±rlÄ±klarÄ± olan bazÄ± nÃ¶ronlarÄ± kaldÄ±rabiliriz, bÃ¶ylece over fitting kaldÄ±rÄ±lÄ±r..

### ğŸ”ƒ KarÅŸÄ±laÅŸtÄ±rma
<p float="left">
    <img src="../res/NNWithoutDropout.JPG" width="300"  />
    <img src="../res/NNWithDropout.JPG" width="300"  />
</p>

> ğŸ¤¸â€â™€ï¸ Dropout iÅŸleminden Ã¶nce ve sonra bir NN

<p float="left">
    <img src="../res/AccuracyWithoutDropOut.JPG" width="300"  />
    <img src="../res/AccuracyWithDropOut.JPG" width="300"  />
</p>

> âœ¨ Dropout iÅŸleminden Ã¶nce ve sonra accuracy durumu

## ğŸ¤” Ne zaman kullanÄ±lmasÄ± pratiktir?
Kendisinden aktardÄ±ÄŸÄ±mÄ±z problem (kaynak) iÃ§in Ã§ok fazla veriye sahip olduÄŸumuzda ve aktardÄ±ÄŸÄ±mÄ±z problem (hedef) iÃ§in genellikle nispeten daha az veri olduÄŸunda pratiktir ğŸ•µï¸â€

**Daha doÄŸrusu:**

`gÃ¶rev A` ve `gÃ¶rev B` iÃ§in, **A'dan B'ye** aktarmak aÅŸaÄŸÄ±daki durumlarda makul olabilir:

* ğŸš© GÃ¶rev A ve gÃ¶rev B aynÄ± Ã§Ä±kÄ±ÅŸa x sahipler ise
* â­ `gÃ¶rev A` iÃ§in `gÃ¶rev B`'ye gÃ¶re daha Ã§ok veriye sahip isek  
* ğŸ” `gÃ¶rev A`'nÄ±n DÃ¼ÅŸÃ¼k seviye Ã¶zellikleri `task B`'yi Ã¶ÄŸrenmek iÃ§in faydalÄ± ise 

## ğŸŒ YazÄ±nÄ±n AslÄ±
- [Burada ğŸ¾](https://dl.asmaamir.com/5-dlstrategies/a-transferlearning)

## ğŸ§ Referanslar
* [More about transfer learning in Tensorflow](https://www.tensorflow.org/tutorials/images/transfer_learning)
* [Understanding Dropout](https://www.youtube.com/watch?v=ARq74QuavAo)
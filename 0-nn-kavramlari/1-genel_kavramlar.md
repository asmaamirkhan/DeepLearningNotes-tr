# ğŸ” Esas Problem

AÅŸaÄŸÄ±da verilen veri seti iÃ§in:

$$[(x^{1},y^{1}), (x^{2},y^{2}), ...., (x^{m},y^{m})]$$

Bizim amacÄ±mÄ±z:

$$\hat{y}^{(i)} \approx y^{(i)}$$

## ğŸ“š Temel Kavramlar ve Notasyonlar

| Kavram | AÃ§Ä±klama |
| :--- | :--- |
| `m` | Veri setindeki Ã¶rnek sayÄ±sÄ± |
| $$x^{(i)}$$ | Veri setindeki `i`'nci Ã¶rnek |
| `Å·` | Tahmin edilen Ã§Ä±ktÄ± |
| KayÄ±p Fonksiyonu _Loss Function_ `ğ“›(Å·, y)` | **Tek** bir Ã¶rnek iÃ§in hata hesaplama fonksiyonu |
| Cost Function _Maliyet Fonksiyonu_ `ğ™¹(w, b)` | TÃ¼m eÄŸitim setinin kayÄ±p fonksiyonlarÄ±nÄ±n ortalamasÄ± |
| Konveks Fonksiyon | Tek bir yerel deÄŸere sahip fonksiyon |
| Konveks Olmayan Fonksiyon | Ã‡ok sayÄ±da farklÄ± yerel deÄŸere sahip fonksiyon |
| Gradyan Ä°niÅŸi _Gradient Descent_ | Maliyet Fonksiyonunun global deÄŸerini bulmak iÃ§in kullanÄ±lan iteratif bir optimizasyon yÃ¶ntemidir |

> BaÅŸka bir deyiÅŸle: Maliyet Fonksiyonu `w` ve `b` veri seti iÃ§in ne kadar iyi olduklarÄ±nÄ± Ã¶lÃ§er. Ona dayanarak, en iyi `w` ve `b` deÄŸerleri, `ğ™¹(w, b)`'Ä± mÃ¼mkÃ¼n olduÄŸunca kÃ¼Ã§Ã¼lten deÄŸerlerdir

## ğŸ“‰ Gradyan Ä°niÅŸi

Gradyan iniÅŸi, tÃ¼revlenebilir bir fonksiyonun yerel minimumunu bulmak iÃ§in birinci dereceden yinelemeli bir optimizasyon algoritmasÄ±dÄ±r. Buradaki ana fikir, mevcut noktada fonksiyonun gradyanÄ±nÄ±n (veya yaklaÅŸÄ±k gradyanÄ±nÄ±n) zÄ±t yÃ¶nÃ¼nde tekrarlanan adÄ±mlar atmaktÄ±r, Ã§Ã¼nkÃ¼ bu en dik iniÅŸ yÃ¶nÃ¼dÃ¼r. DiÄŸer taraftan, gradyan yÃ¶nÃ¼nde adÄ±m atmak, bu fonksiyonun yerel bir maksimumuna yol aÃ§acaktÄ±r; bu prosedÃ¼r de gradyan yÃ¼kseliÅŸi olarak bilinir.

Genel FormÃ¼l:

$$w:=w-\alpha\frac{dJ(w,b)}{dw}$$

$$b:=b-\alpha\frac{dJ(w,b)}{dw}$$

> `Î±` _\(alpha\)_ **Ã–ÄŸrenme HÄ±zÄ±**'dir \(Learning Rate\)

## ğŸ¥½ Ã–ÄŸrenme HÄ±zÄ± \(Learning Rate\)

Model aÄŸÄ±rlÄ±klarÄ± her gÃ¼ncellendiÄŸinde karÅŸÄ±lÄ±k gelen tahmini hata nedeniyle her Gradyan Ä°niÅŸi tekrarÄ±nÄ±n adÄ±mÄ±nÄ±n boyutunu belirleyen pozitif bir skalardÄ±r, bu nedenle bir sinir aÄŸÄ± modelinin bir problemi ne kadar hÄ±zlÄ± veya yavaÅŸ Ã¶ÄŸrendiÄŸini kontrol eder.

### ğŸ€ Ä°yi Ã–ÄŸrenme HÄ±zÄ±

![](../.gitbook/assets/GoodSGD.gif)

### ğŸ’¢ KÃ¶tÃ¼ Ã–ÄŸrenme HÄ±zÄ±

![](../.gitbook/assets/BadSGD.gif)

## ğŸŒ YazÄ±nÄ±n AslÄ±

* [Burada ğŸ¾](https://dl.asmaamir.com/0-nnconcepts/1-generalconcepts)

## ğŸ§ Referanslar

* [Introduction to Artificial Neural Networks \(ANN\)](https://searchenterpriseai.techtarget.com/definition/neural-network)
* [More on Learning Rate](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)
* [Gradient descent - Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)


# âœ¨ Optimizasyon AlgoritmalarÄ±

HÄ±zlÄ± ve iyi optimizasyon algoritmalarÄ±na sahip olmak tÃ¼m iÅŸin verimliliÄŸini hÄ±zlandÄ±rabilir âœ¨

## ğŸ”© BÃ¶lÃ¼mlÃ¼ Gradyan Ä°niÅŸi \(Batch Gradient Descent\)

BÃ¶lÃ¼mlÃ¼ gradyan iniÅŸinde, gradyan iniÅŸinin her yinelemesi iÃ§in maliyet fonksiyonunun gradyanÄ±nÄ± hesaplamak iÃ§in **tÃ¼m** veri kÃ¼mesini kullanÄ±rÄ±z ve ardÄ±ndan aÄŸÄ±rlÄ±klarÄ± gÃ¼ncelleriz.

* TÃ¼m veri kÃ¼mesini hesaplamada kullandÄ±ÄŸÄ±mÄ±zdan dolayÄ± gradyanÄ±n yakÄ±nsamasÄ± yavaÅŸtÄ±r.

### ğŸ© Stokastik Gradyan Ä°niÅŸi \(SGD\)

Stokastik Gradyan Ä°niÅŸi'nde, gradyanÄ± hesaplamak ve aÄŸÄ±rlÄ±klarÄ± **her** bir yinelemeyle gÃ¼ncellemek iÃ§in tek bir veri noktasÄ± veya Ã¶rnek kullanÄ±yoruz, Ã¶nce tamamen rastgele bir veri setini alabilmemiz iÃ§in Ã¶nce veri kÃ¼mesini karÄ±ÅŸtÄ±rmamÄ±z gerekir.

Rastgele Ã¶rnek, global bir minimumda bulunmamÄ±zda yardÄ±mcÄ± olur ve yerel bir minimumda sÄ±kÄ±ÅŸÄ±p kalmaktan kurtarÄ±r.

* BÃ¼yÃ¼k bir veri kÃ¼mesi iÃ§in Ã¶ÄŸrenme Ã§ok daha hÄ±zlÄ± ve yakÄ±nsama Ã§ok hÄ±zlÄ± ğŸš€

### ğŸ”© Mini-BÃ¶lÃ¼mlÃ¼ Gradyan Ä°niÅŸi \(Mini Batch Gradient Descent\)

* Tek bir eÄŸitim Ã¶rneÄŸi yerine, Ã¶rneklerin kÃ¼Ã§Ã¼k bÃ¶lÃ¼mÃ¼ kullanÄ±ldÄ±ÄŸÄ± bir Stokastik Gradyan Ä°niÅŸinin bir varyasyonudur.
* YaygÄ±n olarak kullanÄ±lÄ±r, daha hÄ±zlÄ± yakÄ±nsar ve daha stabildir
* BÃ¶lÃ¼m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ veri kÃ¼mesine baÄŸlÄ± olarak deÄŸiÅŸebilir

> 1 â‰¤ batch-size â‰¤ m, batch-size bir hiper-parametredir â—

### ğŸ”ƒ KarÅŸÄ±laÅŸtÄ±rma

* Ã‡ok bÃ¼yÃ¼k `batch-size` \(m veya m'ye yakÄ±n\): 
  * Her yineleme iÃ§in Ã§ok uzun
* Ã‡ok kÃ¼Ã§Ã¼k `batch-size` \(1 veya 1'e yakÄ±n\)
  * VektÃ¶rleÅŸtirme hÄ±zÄ±nÄ± kaybetme
* Ne Ã§ok bÃ¼yÃ¼k ne Ã§ok kÃ¼Ã§Ã¼k `batch-size`
  * VektÃ¶rleÅŸtirme yapabiliriz
  * Her yineleme iÃ§in iyi hÄ±z
  * En hÄ±zlÄ± \(en iyi\) Ã¶ÄŸrenme ğŸ¤—âœ¨   

### ğŸš© `Batch-Size` SeÃ§mek Ä°Ã§in Kurallar

* KÃ¼Ã§Ã¼k bir veri kÃ¼mesi iÃ§in \(m â‰¤ 2000\) â¡ bÃ¶lÃ¼mlÃ¼ gradyan iniÅŸi kullan
* Tipik mini batch-size: 64, 128, 256, 512, 1024'e kadar
* mini batch-size CPU / GPU belleÄŸine sÄ±ÄŸdÄ±ÄŸÄ±ndan emin ol

> mini batch-size 2 kuvveti olarak seÃ§mek daha iyidir \(daha hÄ±zlÄ±\) \(bellek sorunlarÄ± nedeniyle\) ğŸ§

## ğŸ”© Momentumlu Gradyan Ä°niÅŸi \(Gradient Descent with Momentum\)

Hemen hemen her zaman, gradient descent with momentum standart gradient descent algoritmasÄ±ndan daha hÄ±zlÄ± yakÄ±nsar. Standart gradient descent algoritmasÄ±nda, algoritmayÄ± yavaÅŸlatan bir yÃ¶nde daha bÃ¼yÃ¼k ve baÅŸka bir yÃ¶nde daha kÃ¼Ã§Ã¼k adÄ±mlar atmamÄ±zdÄ±r ğŸ¤•

Bu momentumun geliÅŸtirebileceÄŸi ÅŸeydir, salÄ±nÄ±mÄ± bir yÃ¶nde kÄ±sÄ±tlar, bÃ¶ylece algoritmamÄ±z daha hÄ±zlÄ± yakÄ±nsayabilir. AyrÄ±ca, y yÃ¶nÃ¼nde atÄ±lan adÄ±mlarÄ±n sayÄ±sÄ± sÄ±nÄ±rlÄ± olduÄŸu iÃ§in daha yÃ¼ksek bir Ã¶ÄŸrenme hÄ±zÄ± belirleyebiliriz ğŸ¤—

AÅŸaÄŸÄ±daki resim daha iyi anlatmaktadÄ±r: ğŸ§

![](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/c9ee03241414e86f59a83b656e48738150bfa1bb/.gitbook/assets/gdvsgdm.png)

**FormÃ¼l:**

$$v_{dW} = \beta v_{dW }+ (1-\beta)dW$$

$$v_{db} = \beta v_{db }+ (1-\beta)db$$

$$W = W -\alpha v_{dW}$$

$$b = b -\alpha v_{db}$$

Daha iyi anlamak iÃ§in:

Momentumlu gradyan iniÅŸinde, gradyan iniÅŸii hÄ±zlandÄ±rmaya Ã§alÄ±ÅŸÄ±rken ÅŸunu sÃ¶yleyebiliriz:

* TÃ¼revler hÄ±zlandÄ±rÄ±cÄ±dÄ±r
* v'ler hÄ±zdÄ±r
* _Î²_ sÃ¼rtÃ¼nmedir

## ğŸ”© RMSprop En Ä°yileyicisi

RMSprop optimizer, momentumlu gradyan iniÅŸi algoritmasÄ±na benzer. RMSprop en iyileyicisi salÄ±nÄ±mlarÄ± dikey yÃ¶nde kÄ±sÄ±tlar. Bu nedenle, Ã¶ÄŸrenme hÄ±zÄ±mÄ±zÄ± artÄ±rabiliriz ve algoritmamÄ±z yatay doÄŸrultuda daha hÄ±zlÄ± yaklaÅŸÄ±rken daha bÃ¼yÃ¼k adÄ±mlar atabilir.

RMSprop ve gradyan iniÅŸi arasÄ±ndaki fark, gradyanlarÄ±m nasÄ±l hesaplandÄ±ÄŸÄ±dÄ±r, RMSProp gradyanlarÄ± aÅŸaÄŸÄ±daki formÃ¼lle hesaplar:

$$S_{dW} = \beta S_{dW} + (1-\beta)dW^2$$

$$S_{db} = \beta S_{db} + (1-\beta)db^2$$

$$W = W -\alpha\frac{dW}{\sqrt{S_{dW}}}$$

$$b = b -\alpha\frac{db}{\sqrt{S_{db}}}$$

## âœ¨ Adam En Ä°yileyicisi

AÃ§Ä±lÄ±mÄ±: **ADA**ptive **M**oment estimation

GÃ¼nÃ¼mÃ¼zde yaygÄ±n olarak kullanÄ±lan algoritmadÄ±r, Adam, RMSprop ve Stokastik Gradyan Ä°niÅŸi'nin momentum ile bir kombinasyonu olarak gÃ¶rÃ¼lebilir. RMSprop gibi Ã¶ÄŸrenme hÄ±zÄ±nÄ± Ã¶lÃ§eklendirmek iÃ§in kare gradyanlarÄ± kullanÄ±r ve momentum ile SGD gibi gradyan yerine gradyanÄ±n hareketli ortalamasÄ±nÄ± kullanarak momentumdan yararlanÄ±r.

> Ã–zetle: Adam = RMSProp + GD with momentum + bias correction

$$v_{dW}=\beta_1v_{dW}+ (1-\beta_1)dW$$

$$v_{db}=\beta_1v_{db}+ (1-\beta_1)db$$

$$S_{dW}=\beta_2S_{dW}+ (1-\beta_2)dW^2$$

$$S_{db}=\beta_2S_{db}+ (1-\beta_2)db^2$$

$$v^{corrected}_{dW}=\frac{v_{dW}}{1-\beta^t_1}$$

$$v^{corrected}_{db}=\frac{v_{dW}}{1-\beta^t_1}$$

$$S^{corrected}_{dW}=\frac{S_{dW}}{1-\beta^t_2}$$

$$S^{corrected}_{db}=\frac{S_{db}}{1-\beta^t_2}$$

$$W = W-\alpha \frac{v^{corrected}_{dW}}{\sqrt{S^{corrected}_{dW}}+\epsilon}$$

$$b = b-\alpha \frac{v^{corrected}_{db}}{\sqrt{S^{corrected}_{db}}+\epsilon}$$

> ğŸ˜µğŸ˜µğŸ˜µ

## ğŸ‘©â€ğŸ« Hiper-parametrelerin SeÃ§imi \(Ã¶nerilen deÄŸerler\)

* _Î±_: ayarlanmasÄ± gerekiyor
* _Î²1_: 0.9
* _Î²2_: 0.999
* _Îµ_: $$10^{-8}$$

## ğŸŒ YazÄ±nÄ±n AslÄ±

* [Burada ğŸ¾](https://dl.asmaamir.com/0-nnconcepts/7-optimization)

## ğŸ§ Referanslar

* [Machine learning Gradient Descent](https://medium.com/datadriveninvestor/gradient-descent-5a13f385d403)


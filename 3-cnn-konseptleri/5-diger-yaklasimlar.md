# âœ¨ CNN'leri Kurmak Ä°Ã§in DiÄŸer YaklaÅŸÄ±mlar

| YaklaÅŸÄ±m | AÃ§Ä±klama |
| :--- | :--- |
| ArtÄ±klÄ± AÄŸlar | Derin NN'lerde **kaybolan eÄŸim** sorununu Ã¶nlemek iÃ§in bir yaklaÅŸÄ±m |
| Bire bir konvolÃ¼syon | **Renk KanallarÄ±nda** filtre uygulanmasÄ± |

## ğŸ”„ ArtÄ±klÄ± AÄŸlar \(Residual Networks\)

### ğŸ™„ Problem

Bir sinir aÄŸÄ± eÄŸitiminin her iterasyon sÄ±rasÄ±nda, tÃ¼m aÄŸÄ±rlÄ±klar, mevcut aÄŸÄ±rlÄ±ÄŸa gÃ¶re hata fonksiyonunun kÄ±smi tÃ¼reviyle orantÄ±lÄ± bir gÃ¼ncelleme alÄ±r. EÄŸer gradyan Ã§ok kÃ¼Ã§Ã¼kse, aÄŸÄ±rlÄ±klar etkili bir ÅŸekilde deÄŸiÅŸmeyecek ve sinir aÄŸÄ±nÄ± ileri eÄŸitimden tamamen durdurabilir ğŸ™„ğŸ˜ª. Bu olaya _**Kaybolan EÄŸim** \(vanishing gradients\)_ denir ğŸ™

> BasitÃ§e ğŸ˜…: Ã‡ok yavaÅŸ gradiyan iniÅŸi nedeniyle verilerin derin sinir aÄŸÄ±nÄ±n katmanlarÄ± arasÄ±nda kaybolduÄŸunu sÃ¶yleyebiliriz

ResNet'in ana fikri, aÅŸaÄŸÄ±daki gibi bir veya daha fazla katmanÄ± atlayan **identity shortcut connection** sunmaktÄ±r.

![](../.gitbook/assets/ResNetConcept.png)

### ğŸ™Œ DÃ¼z AÄŸlar vs ResNet'ler

![](../.gitbook/assets/PlainVsRes.jpg)

### ğŸ‘€ GÃ¶rselleÅŸtirme

![](../.gitbook/assets/ResNetVisualization.png)

## ğŸ¤— Avantajlar

* Bloklardan birinin **identitiy** fonksiyonunu Ã¶ÄŸrenmesi kolay
* Performansa zarar vermeden daha derine gidebilir 
  * DÃ¼z NN'lerde, **Kaybolan ve Patlayan Gradyanlar** problemleri nedeniyle, aÄŸÄ±n performansÄ± derinleÅŸtikÃ§e azalmaktadÄ±r. 

## 1ï¸âƒ£ Bire Bir KonvolÃ¼syon \(One By One Convolutions\)

### Propblem \(Yada motivasyon ğŸ¤”\)

_SÄ±kÄ±ÅŸtÄ±rma_ ve Ã§eÅŸitli evriÅŸim uygulayarak giriÅŸlerin boyutunu azaltabiliriz, bu filtreler giriÅŸin gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n yÃ¼ksekliÄŸini ve geniÅŸliÄŸini azaltabilir; peki renk kurallarÄ± ne oluyor? ğŸŒˆ, baÅŸka bir deyiÅŸle **derinliÄŸi** ne oluyor?

### ğŸ¤¸â€â™€ï¸ Ã‡Ã¶zÃ¼m

Bir CNN Ã§Ä±ktÄ±sÄ±nÄ±n derinliÄŸinin giriÅŸte uyguladÄ±ÄŸÄ±mÄ±z filtre sayÄ±sÄ±na eÅŸit olduÄŸunu biliyoruz;

![](../.gitbook/assets/ConvMulti.png)

YukarÄ±daki Ã¶rnekte **2** filtre uyguladÄ±k, bÃ¶ylece Ã§Ä±kÄ±ÅŸ derinliÄŸi **2**

CNN'lerimizi geliÅŸtirmek iÃ§in bu bilgiyi nasÄ±l kullanabiliriz? ğŸ™„

Diyelim ki `28x28x192` boyutlu giriÅŸimiz var, `1x1x192` boyutunda `32` filtre ve [DolgulamalÄ±](https://github.com/asmaamirkhan/DeepLearningNotes-tr/tree/e17776b1b8771d34c5ad3be2b028a41ce37fe32c/3-CNNKonseptleri/1-GenelKavramlar-P2.md#same-convolutions) konvolÃ¼syon uygularsak Ã§Ä±kÄ±ÅŸÄ±mÄ±z `28x28x32` âœ¨

## ğŸŒ YazÄ±nÄ±n AslÄ±

* [Burada ğŸ¾](https://dl.asmaamir.com/3-cnnconcepts/5-otherapproaches)

## ğŸ§ Daha Fazla Oku

* [Detailed ResNets](https://engmrk.com/residual-networks-resnets/)


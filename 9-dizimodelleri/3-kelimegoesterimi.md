# ğŸŒš Kelime GÃ¶sterimi

> Bu dosya yanlÄ±ÅŸ bilgi iÃ§erebilir ğŸ™„â€¼ LÃ¼tfen bir hata bulduÄŸunuzda dÃ¼zeltmem iÃ§in _Pull Request_ aÃ§Ä±n ğŸŒŸ

* One Hot Encoding
* Featurized Representation \(Word Embedding\)
* Word2Vec
* Skip Gram Model
* GloVe \(Global Vectors for Word Representation\)

## ğŸš€ One Hot Encoding

Kelimeleri temsil etmenin bir yolu, bÃ¶ylece onlarÄ± kolayca kullanabilriz

### ğŸ” Ã–rnek

Diyelim ki 10 kelimeden oluÅŸan \(ğŸ¤­\) bir sÃ¶zlÃ¼ÄŸÃ¼mÃ¼z var ve sÃ¶zlÃ¼ÄŸkteki kelimeler:

* Car, Pen, Girl, Berry, Apple, Likes, The, And, Boy, Book.

Ve Bizim $$X^{(i)}$$: **The Girl Likes Apple And Berry**

BÃ¶ylece bu diziyi aÅŸaÄŸÄ±daki gibi temsil edebiliriz ğŸ‘€

```text
Car   -0)  âŒˆ 0 âŒ‰   âŒˆ 0 âŒ‰   âŒˆ 0 âŒ‰   âŒˆ 0 âŒ‰  âŒˆ 0 âŒ‰   âŒˆ 0 âŒ‰ 
Pen   -1)  | 0 |  | 0 |  | 0 |  | 0 |  | 0 |  | 0 |
Girl  -2)  | 0 |  | 1 |  | 0 |  | 0 |  | 0 |  | 0 |
Berry -3)  | 0 |  | 0 |  | 0 |  | 0 |  | 0 |  | 1 |
Apple -4)  | 0 |  | 0 |  | 0 |  | 1 |  | 0 |  | 0 |
Likes -5)  | 0 |  | 0 |  | 1 |  | 0 |  | 0 |  | 0 |
The   -6)  | 1 |  | 0 |  | 0 |  | 0 |  | 0 |  | 0 |
And   -7)  | 0 |  | 0 |  | 0 |  | 0 |  | 1 |  | 0 |
Boy   -8)  | 0 |  | 0 |  | 0 |  | 0 |  | 0 |  | 0 |
Book  -9)  âŒŠ 0 âŒ‹   âŒŠ 0 âŒ‹   âŒŠ 0 âŒ‹   âŒŠ 0 âŒ‹  âŒŠ 0 âŒ‹   âŒŠ 0 âŒ‹
```

Dizileri bu ÅŸekilde temsil ederek verileri sinir aÄŸlarÄ±na aktarabiliriz âœ¨

### ğŸ™„ Dezavantaj

* EÄŸer sÃ¶zlÃ¼ÄŸÃ¼mÃ¼z 10.000 kelimeden oluÅŸuyorsa, her vektÃ¶r 10.000 boyutlu olacaktÄ±r ğŸ¤• 
* Bu gÃ¶sterim anlamsal Ã¶zellikleri yakalayamaz ğŸ’”

## ğŸ Featurized Representation \(Word Embedding\)

* Kelimeleri cinsiyet, yaÅŸ, kraliyet, yemek, maliyet, bÃ¼yÃ¼klÃ¼k vb. gibi Ã¶zelliklerle iliÅŸkilendirerek temsil etmek
* Her Ã¶zellik \[-1, 1\] arasÄ±nda bir aralÄ±k olarak temsil edilir 
* BÃ¶ylece her kelime bu Ã¶zelliklerin bir vektÃ¶rÃ¼ olarak gÃ¶sterilebilir
  * Her vektÃ¶rÃ¼n boyutu, seÃ§tiÄŸimiz Ã¶zelliklerin sayÄ±sÄ±na baÄŸlÄ±dÄ±r

### ğŸ”¢ Embedded Matrix

For a given word _w_, the embedding matrix _E_ is a matrix that maps its 1-hot representation $$o_w$$ to its embedding $$e_w$$ as follows: $$e_w=Eo_w$$

> Ã‡eviremedim ğŸ˜¢

### ğŸ€ Avantajlar

* **Benzer** anlama sahip olan kelimelerin **benzer** bir gÃ¶sterimi vardÄ±r.
* Bu model anlamsal \(semantic\) Ã¶zellikleri yakalayabilir âœ¨ 
* VektÃ¶rler one-hot gÃ¶sterimindeki vektÃ¶rlerden daha kÃ¼Ã§Ã¼ktÃ¼r.

> TODO: Subtracting vectors of oppsite words

## ğŸ”„ Word2Vec

* Word2vec, Verilen bir kelimenin diÄŸer kelimelerle Ã§evrili olma olasÄ±lÄ±ÄŸÄ±nÄ± tahmin ederek kelime embedding'lerini Ã¶ÄŸrenme stratejisidir.
* Bu, daha sonra aldÄ±ÄŸÄ±mÄ±z **pencere boyutu**na baÄŸlÄ± olan baÄŸlam \(context\) ve hedef \(target\) kelime Ã§iftleri yaparak yapÄ±lÄ±r.
  * **Pencere boyutu**: BaÄŸlam sÃ¶zcÃ¼ÄŸÃ¼nÃ¼n soluna ve saÄŸÄ±na bakan bir parametredir

![](../.gitbook/assets/CTPairs.png)

> _pencere boyutu = 2_ ile _baÄŸlam-hedef_ Ã§iftleri oluÅŸturma ğŸ™Œ

## Skip Gram Model

Skip-gram word2vec modeli verilen herhangi bir _t_ hedef kelimesinin _c_ gibi bir baÄŸlam kelimesi ile gerÃ§ekleÅŸme olasÄ±lÄ±ÄŸÄ±nÄ± deÄŸerlendirerek kelime embedding'lerini Ã¶ÄŸrenen denetimli bir Ã¶ÄŸrenme gÃ¶revidir. _P\(t\|c\)_ olasÄ±lÄ±ÄŸÄ± da aÅŸaÄŸÄ±daki ÅŸekilde hesaplanÄ±r:

$$P(t|c)=\frac{exp(\theta^T_te_c)}{\sum_{j=1}^{|V|}exp(\theta^T_je_c)}$$

> Not: Softmax bÃ¶lÃ¼mÃ¼nÃ¼n paydasÄ±ndaki tÃ¼m kelime sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ toplamak, bu modeli hesaplama aÃ§Ä±sÄ±ndan maliyetli kÄ±lar

### ğŸš€ One Hot Rep. vs Word Embedding

![](../.gitbook/assets/OneHotVsWordEmbedding.PNG)

## ğŸ§¤ GloVe

Kelime gÃ¶sterimi iÃ§in Global vektÃ¶rler tanÄ±mÄ±nÄ±n kÄ±saltÄ±lmÄ±ÅŸ hali olan GloVe, eÅŸzamanlÄ± bir _X_ matrisi kullanan ki burada her bir $$X_{ij}$$, bir hedefin bir _j_ baÄŸlamÄ±nda gerÃ§ekleÅŸtiÄŸi sayÄ±sÄ±nÄ± belirten bir kelime gÃ¶mme tekniÄŸidir. Maliyet fonksiyonu _J_ aÅŸaÄŸÄ±daki gibidir:

$$J(\theta)=\frac{1}{2}\sum_{i,j=1}^{|V|}f(X_{ij})(\theta^T_ie_j+b_i+b'_j-log(X_{ij}))^2$$

_f_, $$X_{ij}=0$$ âŸ¹ $$f(X_{ij})$$ = 0 olacak ÅŸekilde bir aÄŸÄ±rlÄ±klandÄ±rma fonksiyonudur.

Bu modelde _e_ ve _Î¸_'nÄ±n oynadÄ±ÄŸÄ± simetri gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, $$e^{(final)}_w$$'nin kelime embedding'i ÅŸÃ¶yle ifade edilir:

$$e^{(final)}_w=\frac{e_w+\theta_w}{2}$$

## ğŸ‘©â€ğŸ« Word Embeddings'in Ã–zeti

* EÄŸer bu ilk denemen ise, yapÄ±lmÄ±ÅŸ ve gerÃ§ekten en iyi ÅŸekilde Ã§alÄ±ÅŸan Ã¶nceden eÄŸitilmiÅŸ bir modeli indirmeyi denemelisin.
* Yeterli veriye sahipsen, mevcut algoritmalardan birini uygulamaya Ã§alÄ±ÅŸabilirsin.
* Kelime embedding'lerinin eÄŸitilmesi Ã§ok maliyetli bir iÅŸlem olduÄŸu iÃ§in, Ã§oÄŸu ML'ciler Ã¶nceden eÄŸitilmiÅŸ embedding'ler kullanÄ±rlar.

## ğŸŒ YazÄ±nÄ±n AslÄ±

* [Burada ğŸ¾](https://dl.asmaamir.com/9-sequencemodels/3-wordrepres)

## ğŸ§ Referanslar

* [Recurrent Neural Networks Cheatsheet âœ¨](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
* [NLP â€” Word Embedding & GloVe](https://medium.com/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6)


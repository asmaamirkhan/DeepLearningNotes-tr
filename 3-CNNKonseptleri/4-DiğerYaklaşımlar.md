# âœ¨ CNN'leri Kurmak Ä°Ã§in DiÄŸer YaklaÅŸÄ±mlar 
| YaklaÅŸÄ±m   | AÃ§Ä±klama      |
| ---------- |---------------|
| Residual Networks | Derin NN'lerde **vanishing gradient** sorununu Ã¶nlemek iÃ§in bir yaklaÅŸÄ±m |
| One By One Convolution | **Renk KanallarÄ±nda** filtre uygulanmasÄ± |

## ğŸ”„ Residual Networks
### ğŸ™„ Problem 
Bir sinir aÄŸÄ± eÄŸitiminin her itersyon sÄ±rasÄ±nda, tÃ¼m aÄŸÄ±rlÄ±klar, mevcut aÄŸÄ±rlÄ±ÄŸa gÃ¶re hata fonksiyonunun kÄ±smi tÃ¼reviyle orantÄ±lÄ± bir gÃ¼ncelleme alÄ±r.
EÄŸer gradyan Ã§ok kÃ¼Ã§Ã¼kse, aÄŸÄ±rlÄ±klar etkili bir ÅŸekilde deÄŸiÅŸmeyecek ve sinir aÄŸÄ±nÄ± ileri eÄŸitimden tamamen durdurabilir ğŸ™„ğŸ˜ª.

During each iteration of training a neural network, all weights receive an update proportional to the partial derivative of the error function with respect to the current weight. If the gradient is very small then the weights will not be change effectively and it may completely stop the neural network from further training ğŸ™„ğŸ˜ª. Bu olaya _vanishing gradients_ denir ğŸ™

> BasitÃ§e ğŸ˜…: Ã‡ok yavaÅŸ _gradient descent_ nedeniyle verilerin derin sinir aÄŸÄ±nÄ±n katmanlarÄ± arasÄ±nda kaybolduÄŸunu sÃ¶yleyebiliriz

ResNet'in ana fikri, aÅŸaÄŸÄ±daki gibi bir veya daha fazla katmanÄ± atlayan **identity shortcut connection** sunmaktÄ±r.

<img src="../res/ResNetConcept.png" width="300"  />

### ğŸ™Œ DÃ¼z AÄŸlar vs ResNet'ler
<img src="../res/PlainVsRes.jpg" width="400"  />

### ğŸ‘€ GÃ¶rselleÅŸtirme
<img src="../res/ResNetVisualization.png" width="600"  />

## ğŸ¤— Avantajlar
- Bloklardan birinin **identitiy** fonksiyonunu Ã¶ÄŸrenmesi kolay
- Performansa zarar vermeden daha derine gidebilir 
  - DÃ¼z NN'lerde, **vanishing and exploding gradients** problemleri nedeniyle, aÄŸÄ±n performansÄ± derinleÅŸtikÃ§e azalmaktadÄ±r. 

## 1ï¸âƒ£ One By One Convolutions
### Propblem (Yada motivasyon ğŸ¤”)
We can reduce the size of inputs by applying pooling and various convolution, these filteres can reduce the height and the width of the input image, what about color channels ğŸŒˆ, in other words; what about the **depth**?

_Pooling_ ve Ã§eÅŸitli evriÅŸim uygulayarak giriÅŸlerin boyutunu azaltabiliriz, bu filtreler giriÅŸin gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼n yÃ¼ksekliÄŸini ve geniÅŸliÄŸini azaltabilir; peki renk kurallarÄ± ne oluyor? ğŸŒˆ, baÅŸka bir deyiÅŸle **derinliÄŸi** ne oluyor?

### ğŸ¤¸â€â™€ï¸ Ã‡Ã¶zÃ¼m
Bir CNN Ã§Ä±ktÄ±sÄ±nÄ±n derinliÄŸinin giriÅŸte uyguladÄ±ÄŸÄ±mÄ±z filtre sayÄ±sÄ±na eÅŸit olduÄŸunu biliyoruz;

<img src="../res/ConvMulti.png" width="400"  />

YukarÄ±daki Ã¶rnekte **2** filtre uyguladÄ±k, bÃ¶ylece Ã§Ä±kÄ±ÅŸ derinliÄŸi **2**

CNN'lerimizi geliÅŸtirmek iÃ§in bu bilgiyi nasÄ±l kullanabiliriz? ğŸ™„

Let's say that we have a `28x28x192` dimensional input, if we apply `32` filters at `1x1x192` dimension and [SAME](./1-CommonConcepts-P2.md#same-convolutions) padding our output will become `28x28x32` âœ¨

Diyelim ki `28x28x192` boyutlu giriÅŸimiz var, `1x1x192` boyutunda `32` filtre ve [SAME](./1-GenelKavramlar-P2.md#same-convolutions) padding uygularsak Ã§Ä±kÄ±ÅŸÄ±mÄ±z `28x28x32` âœ¨

## ğŸŒ YazÄ±nÄ±n AslÄ±
- [Burada ğŸ¾](https://dl.asmaamir.com/3-cnnconcepts/4-otherapproaches)

## ğŸ§ Daha Fazla Oku
- [Detailed ResNets](https://engmrk.com/residual-networks-resnets/)
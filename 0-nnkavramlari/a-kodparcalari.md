# ğŸ‘©â€ğŸ’» Yapay Sinir AÄŸlarÄ±yla ilgili Python Kod ParÃ§alarÄ±

## ğŸ“š ML'de Genel Kod ParÃ§alarÄ±

### ğŸ’¥ Sigmoid Fonksiyonu

{% tabs %}
{% tab title="â— FormÃ¼l" %}
$$sigmoid(x)=\frac{1}{1+exp(-x)}$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def sigmoid(x):
    """
    ArgÃ¼manlar:
    x -- Skaler, dizi veya matris

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    result -- sigmoid(x)
    """

    result = 1 /( 1 + np.exp(-x) )

    return result
```
{% endtab %}
{% endtabs %}

### ğŸš€ Sigmoid Gradient

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
_backpropagation_ kullanarak loss fonksiyonlarÄ±nÄ± optimize etmek iÃ§in _gradient_'leri hesaplayan fonksiyon
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$\sigma^{'}(x)=\sigma(x)(1-\sigma(x))$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
    def sigmoid_derivative(x):
    """
    Sigmoid fonksiyonunun gradient'Ä±nÄ± (eÄŸim veya tÃ¼rev olarak da adlandÄ±rÄ±lÄ±r), x girdisine gÃ¶re hesaplar
    ArgÃ¼manlar:
    x -- scaler veya Numpy dizisi

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    ds -- Hesaplanan gradient.
    """

    s = 1 / (1 + np.exp(-x))
    ds = s * (1 - s)

    return ds
```
{% endtab %}
{% endtabs %}

### ğŸ‘©â€ğŸ”§ Dizileri \(Veya resimleri\) Yeniden Åekillendirme

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
    def arr2vec(arr, target):
     """
    ArgÃ¼manlar:
    image -- (length, height, depth) boyutunda bir Numpy dizisi

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    v -- (length*height*depth, 1) boyutunda bir vektÃ¶r
    """

    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)

    return v
```
{% endtab %}
{% endtabs %}

### ğŸ’¥ SatÄ±rlarÄ± Normalize Etme

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
X'in her satÄ±r vektÃ¶rÃ¼nÃ¼ normuna gÃ¶re bÃ¶lme.
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$Normalization(x)=\frac{x}{||x||}$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def normalizeRows(x):
    """
    ArgÃ¼manlar:
    x -- (n, m) boyutunda bir Numpy dizisi

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    x -- Normalize edilmiÅŸ (satÄ±ra gÃ¶re) Numpy matrisi.
    """

    # NormlarÄ± hesaplama
    x_norm = np.linalg.norm(x, axis=1, keepdims=True)

    # x'i normuna bÃ¶lme 
    x = x / x_norm

    return x
```
{% endtab %}
{% endtabs %}

### ğŸ¨ Softmax Fonksiyonu

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
AlgoritmanÄ±n iki veya daha fazla sÄ±nÄ±fÄ± sÄ±nÄ±flandÄ±rmasÄ± gerektiÄŸinde kullanÄ±lan normalleÅŸtirme fonksiyonu
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$Softmax(x_i)=\frac{exp(x_i)}{\sum_{j}exp(x_j)}$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
 def softmax(x):
    """X giriÅŸinin her satÄ±rÄ± iÃ§in softmax deÄŸerini hesaplar.

    ArgÃ¼manlar:
    x -- (n,m) boyutunda bir matris

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    s -- (X, m) ÅŸeklindeki softmax x deÄŸerine eÅŸit bir matris matrisi
    """

    # Exp () element-wise komutunu x'e uygulama
    x_exp = np.exp(x)

    # X_exp'nin her satÄ±rÄ±nÄ± toplayan bir vektÃ¶r x_sum oluÅŸturma
    x_sum = np.sum(x_exp, axis=1, keepdims=True)

    # Softmax (x)'in x_exp'i x_sum ile bÃ¶lerek hesaplanmasÄ±.
    # numpy broadcasting otomatik olarak kullanÄ±lacak
    s = x_exp / x_sum

    return s
```
{% endtab %}
{% endtabs %}

### ğŸ¤¸â€â™€ï¸ L1 Loss Fonksiyonu

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
KayÄ±p, modelin performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lÄ±r. KayÄ±p ne kadar bÃ¼yÃ¼kse, tahminlerin \(Å·\) gerÃ§ek deÄŸerlerden \(y\) o kadar farklÄ± olmasÄ±dÄ±r. Derin Ã¶ÄŸrenmede, modeli eÄŸitmek ve maliyeti en aza indirmek iÃ§in _Gradient Descent_ gibi optimizasyon algoritmalarÄ± kullanÄ±yoruz.
{% endtab %}

{% tab title="â— FormÃ¼la" %}
$$L_1(\hat{y},y)=\sum_{i=0}^{m}(|y^{(i)}-\hat{y}^{(i)}|)$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def L1(yhat, y):
    """
    ArgÃ¼manlar:
    yhat --  m boyutunda bir vektÃ¶r (tahmin edilen etiketler)
    y -- m boyutunda bir vektÃ¶r (doÄŸru etiketler)

    DÃ¶nÃ¼ÅŸ deÄŸeri: 
    loss -- yanda tanÄ±mlanan L1 fonksiyonunun deÄŸeri
    """

    loss = np.sum(np.abs(y - yhat))

    return loss
```
{% endtab %}
{% endtabs %}

### ğŸ¤¸â€â™‚ï¸ L2 Loss Fonksiyonu

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
KayÄ±p, modelin performansÄ±nÄ± deÄŸerlendirmek iÃ§in kullanÄ±lÄ±r. KayÄ±p ne kadar bÃ¼yÃ¼kse, tahminlerin \(Å·\) gerÃ§ek deÄŸerlerden \(y\) o kadar farklÄ± olmasÄ±dÄ±r. Derin Ã¶ÄŸrenmede, modeli eÄŸitmek ve maliyeti en aza indirmek iÃ§in _Gradient Descent_ gibi optimizasyon algoritmalarÄ± kullanÄ±yoruz.
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$L_2(\hat{y},y)=\sum_{i=0}^{m}(y^{(i)}-\hat{y}^{(i)})^2$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def L2(yhat, y):
    """
    ArgÃ¼manlar:
    yhat -- m boyutunda bir vektÃ¶r(tahmin edilen etiketler)
    y -- m boyutunda bir vektÃ¶r(doÄŸru etiketler)

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    loss -- yanda tanÄ±mlanan L1 fonksiyonunun deÄŸeri
    """

    loss = np.sum((y - yhat) ** 2)

    return loss
```
{% endtab %}
{% endtabs %}

### ğŸƒâ€â™€ï¸ YayÄ±lma Fonksiyonu \(_Propagation Function_\)

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
Parametreleri Ã¶ÄŸrenmek iÃ§in "ileri" ve "geri" yayÄ±lma adÄ±mlarÄ±nÄ± yapmak.
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$\frac{\partial J}{\partial w}=\frac{1}{m}X(A-Y)^T$$

$$\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^{m}(a^{(i)}-y^{(i)})$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def propagate(w, b, X, Y):
    """
    Cost fonksiyonunu ve gradient'leri hasaplamanÄ±n implementasyonu

    ArgÃ¼manlar:
    w -- aÄŸÄ±rlÄ±klar, (num_px * num_px * 3, 1) boyutunda bir Numpy dizisi
    b -- Ã¶nyargÄ± (bias), bir skalerdir
    X -- (num_px * num_px * 3, Ã¶rnek sayÄ±sÄ±) boyutunda veriler
    Y -- doÄŸru etiket vektÃ¶rÃ¼ (kedi deÄŸilse 0, kediyse 1), (1, Ã¶rnek sayÄ±sÄ±) boyutunda

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    cost -- lojistik regresyon iÃ§in negatif log olabilirlik cost'u
    dw -- w'ye gÃ¶re kaybÄ±n gradyanÄ±, bu nedenle w ile aynÄ± boyutta
    db -- b'ye gÃ¶re kaybÄ±n gradyanÄ±, bu nedenle b ile aynÄ± boyutta

    """

    m = X.shape[1]

    # Ä°LERÄ° YAYILMA (Xâ€™DEN MALÄ°YETE)

    # aktivasyonu hesaplama
    A = sigmoid( np.dot(w.T, X) + b ) 

    # cost'u hesaplama
    cost = - np.sum( Y * np.log(A) + (1-Y) * np.log(1 - A) ) / m 

    # GERÄ° YAYILMA (GRAD'Ä± BULMAK Ä°Ã‡Ä°N)

    dw = (np.dot(X,(A-Y).T))/m
    db = np.sum(A-Y)/m

    grads = {"dw": dw,
             "db": db}

    return grads, cost
```
{% endtab %}
{% endtabs %}

### ğŸ’« Gradyan Ä°niÅŸi _Gradient Descent_ \(Optimizasyon\)

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
AmaÃ§, maliyet fonksiyonunu _J_'yi en aza indirerek _Ï‰_ ve _b_'yi Ã¶ÄŸrenmektir.
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$w=w-\alpha dw$$

Î± Ã¶ÄŸrenme hÄ±zÄ±dÄ±r _learning rate_
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    gradient descent algoritmasÄ± Ã§alÄ±ÅŸtÄ±rarak w ve b'yi optimize eder

    ArgÃ¼manlar:
    w -- aÄŸÄ±rlÄ±klar, (num_px * num_px * 3, 1) boyutunda bir Numpy dizisi
    b -- Ã¶nyargÄ± (bias), bir skalerdir
    X -- (num_px * num_px * 3, kayÄ±t sayÄ±sÄ±) boyutunda veriler
    Y -- doÄŸru etiket vektÃ¶rÃ¼ (kedi deÄŸilse 0, kediyse 1), (1, Ã¶rnek sayÄ±sÄ±) boyutunda
    num_iterations -- optimizasyon dÃ¶ngÃ¼sÃ¼nÃ¼n iterasyon sayÄ±sÄ±
    learning_rate -- gradient descent'in Ã¶ÄŸrenme hÄ±zÄ±
    print_cost -- True ise kaybÄ± her 100 adÄ±m yazdÄ±rÄ±r

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    params -- aÄŸÄ±rlÄ±klarÄ± ve bias'Ä± iÃ§eren bir dictionary
    grads -- cost fonksiyonuna gÃ¶re aÄŸÄ±rlÄ±klarÄ± ve bias'Ä± iÃ§eren bir dictionary
    costs -- optimizasyon esnasÄ±nda bÃ¼tÃ¼n kayÄ±p deÄŸerlerini iÃ§eren bir list, Ã¶ÄŸrenme eÄŸrisini Ã§izerken kullanÄ±lacak
    """

    costs = []

    for i in range(num_iterations):


        # Cost ve gradient'Ä± hesaplama
        grads, cost = propagate(w, b, X, Y)

        # tÃ¼revleri grad'lardan elde etme 
        dw = grads["dw"]
        db = grads["db"]

        # kuralÄ± gÃ¼ncelleme
        w = w - learning_rate*dw
        b = b - learning_rate*db

        # cost'larÄ± kaydetme
        if i % 100 == 0:
            costs.append(cost)

        # kaybÄ± her 100 iterasyonda yazdÄ±rÄ±r (opsiyonel)
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads, costs
```
{% endtab %}
{% endtabs %}

## ğŸ•¸ Basit Bir NN Temel Kod ParÃ§larÄ±

2 katmanlÄ± aÄŸÄ±n fonksiyonlarÄ±

> GiriÅŸ katmanÄ±, 1 gizli katman ve Ã§Ä±kÄ±ÅŸ katmanÄ±

### ğŸš€ Parametreleri BaÅŸlatma _Initialization_

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
`W`'leri ve `b`'leri baÅŸlatma konusunda, `W`'ler **SimetriÄŸi kÄ±rma** adÄ±na rastgele deÄŸerlerle baÅŸlatmalÄ±yÄ±z, `b`'yi ise sÄ±fÄ±r olarak baÅŸlatabiliriz.
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ« Kod" %}
```python
def initialize_parameters(n_x, n_h, n_y):
    """
    ArgÃ¼manlar:
    n_x -- giriÅŸ katmanÄ±nÄ±n boyutu
    n_h -- gizli katmanÄ±n boyutu
    n_y -- Ã§Ä±kÄ±ÅŸ katmanÄ±nÄ±n boyutu

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    params -- parametreleri iÃ§eren bir:
                    W1 -- (n_h, n_x) boyutundaki aÄŸÄ±rlÄ±klar matrisi
                    b1 -- (n_h, 1) boyutundaki bias vektÃ¶rÃ¼
                    W2 -- (n_y, n_h) boyutundaki aÄŸÄ±rlÄ±klar matrisi
                    b2 -- (n_y, 1) boyutundaki bias vektÃ¶rÃ¼
    """
    # deÄŸerleri kÃ¼Ã§Ã¼ltmek iÃ§in 0.01 ile Ã§arpma
    W1 = np.random.randn(n_h,n_x) * 0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h) * 0.01
    b2 = np.zeros((n_y,1))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters
```
{% endtab %}
{% endtabs %}

### â© Ä°leri YayÄ±lma

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
Her katman giriÅŸ verilerini alÄ±r, aktivasyon fonksiyonuna gÃ¶re iÅŸler ve sonraki katmana geÃ§irir
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def forward_propagation(X, parameters):
    """
    ArgÃ¼manlar:
    X -- (n_x, m) boyutundaki giriÅŸ verileri
    parameters -- parametreleri iÃ§eren dictionary (baÅŸlatma fonksiyonunun Ã§Ä±kÄ±ÅŸ deÄŸeri)

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    A2 -- ikinci aktivasyonun sigmoid Ã§Ä±kÄ±ÅŸÄ±
    cache -- "Z1", "A1", "Z2" and "A2" deÄŸerlerini iÃ§eren dictionary
    """

    # parameters'den parametreleri elde etme
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache
```
{% endtab %}
{% endtabs %}

### ğŸš© Maliyet Fonksiyonu _Cost_

{% tabs %}
{% tab title=".ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
Ã‡Ä±kÄ±ÅŸ katmanÄ± nedeniyle ayarlanan **tÃ¼m** eÄŸitimin _loss_ fonksiyonlarÄ±nÄ±n ortalamasÄ±
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$J=-\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(a^{[2](i)}) + (1-y^{(i)}log(1-a^{[2](i)})))$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def compute_cost(A2, Y):
    """
    FormÃ¼lde verilen cross-entropy maliyetini hesaplar

    ArgÃ¼manlar:
    A2 -- ikinci aktivasyonun sigmoid Ã§Ä±kÄ±ÅŸÄ±, (1, Ã¶rnek sayÄ±sÄ±) boyutunda
    Y -- "true" etiket vektÃ¶rÃ¼ (1, Ã¶rnek sayÄ±sÄ±) boyutunda  

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    cost -- formÃ¼lde verilen cross-entropy maliyeti

    """

    # Ã¶rnek sayÄ±sÄ±
    m = Y.shape[1] 

    # cross-entropy maliyetini hesaplama
    logprobs = np.multiply(np.log(A2), Y) + (1 - Y) * np.log(1 - A2)
    cost = - np.sum(logprobs) / m
    cost = float(np.squeeze(cost))  

    return cost
```
{% endtab %}
{% endtabs %}

### âª Geri YayÄ±lma

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
AÄŸÄ±rlÄ±klarÄ±n uygun ÅŸekilde ayarlanmasÄ± daha dÃ¼ÅŸÃ¼k hata oranlarÄ±nÄ± garanti eder ve modellemeyi genellemesini artÄ±rarak gÃ¼venilir kÄ±lar.
{% endtab %}

{% tab title="â— FormÃ¼l" %}
![](../.gitbook/assets/summarygd.PNG)
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def backward_propagation(parameters, cache, X, Y):
    """
    Daha Ã¶nce verilen talimatlarÄ± kullanarak geriye doÄŸru yayÄ±lÄ±mÄ± uygular

    ArgÃ¼manlar:
    parameters -- parametrelerimizi iÃ§eren dictionary 
    cache -- "Z1", "A1", "Z2" and "A2" deÄŸerlerini iÃ§eren dictionary
    X -- (2, Ã¶rnek sayÄ±sÄ±) boyutundaki giriÅŸ verileri
    Y -- "true" etiket vektÃ¶rÃ¼, (1, Ã¶rnek sayÄ±sÄ±) boyutunda

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    grads -- farklÄ± parametrelere gÃ¶re gradyanlarÄ± iÃ§eren dictionary
    """
    m = X.shape[1]

    # parameters'dan W1 ve W2'yi elde etme
    W1 = parameters['W1']
    W2 = parameters['W2']

    # cache'den A1 ve A2'yi elde etme
    A1 = cache['A1']
    A2 = cache['A2']

    # Geri yayÄ±lma: calculating dW1, db1, dW2, db2 hesaplama 
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis = 1, keepdims = True) / m
    dZ1 = np.dot(W2.T, dZ2) * (1 - A1 ** 2)
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis = 1, keepdims = True) / m

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads
```
{% endtab %}
{% endtabs %}

### ğŸ”ƒ Parametreleri GÃ¼ncelleme

{% tabs %}
{% tab title="ğŸ‘©â€ğŸ« AÃ§Ä±klama" %}
_Gradient descent_'i tamamlamak iÃ§in Ã¶ÄŸrenme hÄ±zÄ±na baÄŸlÄ± olarak parametrelerin gÃ¼ncellenmesi
{% endtab %}

{% tab title="â— FormÃ¼l" %}
$$\theta := \theta - \alpha \frac{\partial J}{\partial \theta}$$
{% endtab %}

{% tab title="ğŸ‘©â€ğŸ’» Kod" %}
```python
def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    Daha Ã¶nce verilen gradient descent gÃ¼ncelleme kuralÄ±nÄ± kullanarak parametreleri gÃ¼nceller

    ArgÃ¼manlar:
    parameters -- parametrelerimizi iÃ§eren dictionary 
    grads -- gradyanlarÄ±mÄ±zÄ± iÃ§eren dictionary 

    DÃ¶nÃ¼ÅŸ deÄŸeri:
    parameters -- gÃ¼ncellenmiÅŸ parametreleri iÃ§eren dictionary 
    """
    # "parameters"'dan parametreleri elde etme
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    # "grads"dan gradyanlarÄ± elde etme
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']

    # KuralÄ± her parametre iÃ§in gÃ¼ncelleme
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters
```
{% endtab %}
{% endtabs %}

## ğŸŒ YazÄ±nÄ±n AslÄ±

* [Burada ğŸ¾](https://dl.asmaamir.com/0-nnconcepts/a-codesnippets)


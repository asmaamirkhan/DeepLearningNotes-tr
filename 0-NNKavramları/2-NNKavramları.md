# ğŸ“š Yapay Sinir AÄŸlarÄ±nÄ±n KavramlarÄ±

Yapay Sinir AÄŸlarÄ±nÄ±n temel kavramlarÄ±

## ğŸ­ Temel Sinir AÄŸÄ±

<img src="../res/BasicANN.png" width="300"  />

> **Convention:** GiriÅŸ katmanÄ± sayÄ±lmadÄ±ÄŸÄ±ndan gÃ¶rÃ¼ntÃ¼deki NN 2 kat NN olarak adlandÄ±rÄ±lÄ±r ğŸ“¢â—

## ğŸ“š Terimler

| Terim            | AÃ§Ä±klama      |
| ---------------  |---------------|
| GiriÅŸ KatmanÄ±    | NN'nin giriÅŸlerini iÃ§eren katmandÄ±r |
| Gizli Katman     | HesaplamalÄ± iÅŸlemlerin yapÄ±ldÄ±ÄŸÄ± katman |
| Ã‡Ä±kÄ±ÅŸ KatmanÄ±    | NN'nin son katmanÄ± ve tahmin edilen deÄŸerin _yÌ‚_ Ã¼retilmesinden sorumludur |
| NÃ¶ron            | Bir matematik fonksiyonu iÃ§in bir yer tutucu, girdilere bir fonksiyon uygular ve Ã§Ä±ktÄ± saÄŸlar |
| Aktivasyon Fonksiyonu | DÃ¶nÃ¼ÅŸÃ¼mler uygulayarak bir dÃ¼ÄŸÃ¼mÃ¼n giriÅŸ sinyalini bir Ã§Ä±kÄ±ÅŸ sinyaline dÃ¶nÃ¼ÅŸtÃ¼ren bir fonksiyon |
| Shallow NN       | Az sayÄ±da gizli katmana sahip NN (bir veya iki)  |
| Deep NN          | Ã‡ok sayÄ±da gizli katmanÄ± olan NN |
| n<sup>[l]</sup>  | _l_ katmanÄ±ndaki nÃ¶ron sayÄ±sÄ± |


## ğŸ§  What does an artificial neuron do?
GiriÅŸinin aÄŸÄ±rlÄ±klÄ± toplamÄ±nÄ± hesaplar, _bias_ ekler ve ardÄ±ndan bir aktivasyon fonksiyonu nedeniyle nÃ¶ronun tetiklenip tetiklenmeyeceÄŸine karar verir.
> My detailed notes on activation functions are [here](https://github.com/asmaamirkhan/DeepLearningNotes/tree/master/6-NNConcepts/3-ActivationFunctions.md) ğŸ‘©â€ğŸ«




## ğŸ‘©â€ğŸ”§ Parametreler Boyut KontrolÃ¼

| Parametre        | Boyut     |
| ---------------  |---------------|
| w<sup>[<i>l</i>]</sup>   |  (n<sup>[<i>l</i>]</sup>,n<sup>[<i>l-1</i>]</sup>) |
| b<sup>[<i>l</i>]</sup>   |  (n<sup>[<i>l</i>]</sup>,1) |
| dw<sup>[<i>l</i>]</sup>  |  (n<sup>[<i>l</i>]</sup>,n<sup>[<i>l-1</i>]</sup>) |
| db<sup>[<i>l</i>]</sup>  |  (n<sup>[<i>l</i>]</sup>,1) |


> Bu boyutlarÄ±n doÄŸru olduÄŸundan emin olmak, daha iyi ve hatasÄ±z ğŸ› kodlar yazmamÄ±za yardÄ±mcÄ± olur.

## ğŸˆ Summary of Forward Propagation Process

|                  |                 |
| ---------------- | --------------- |
| **GiriÅŸ:**       |  a<sup>[<i>l</i>-1]</sup> |
| **Ã‡Ä±kÄ±ÅŸ:**       |  a<sup>[<i>l</i>]</sup>, chache (z<sup>[<i>l</i>]</sup>) |

**VektÃ¶rize EdilmiÅŸ Denklemler:**

<img src="../res/formulas/ForwardProp.png" height="80"  />

## ğŸˆ Summary of Back Propagation Process

|                  |                 |
| ---------------- | --------------- |
| **GiriÅŸ:**       |  da<sup>[<i>l</i>]</sup> |
| **Ã‡Ä±kÄ±ÅŸ :**      | da<sup>[<i>l</i>-1]</sup>, dW<sup>[<i>l</i>]</sup>, db<sup>[<i>l</i>]</sup> |

**VektÃ¶rize EdilmiÅŸ Denklemler:**

<img src="../res/formulas/BackProp1.png" height="30"  />
<br>
<img src="../res/formulas/BackProp2.png" height="50"  />
<br>
<img src="../res/formulas/BackProp3.png" height="50"  />
<br>
<img src="../res/formulas/BackProp4.png" height="30"  />

## â°â° To Put Forward Prop. and Back Prop. Together

<img src="../res/ForBackSummary.png" width="500"  />

> ğŸ˜µğŸ¤•

## âœ¨ Parametreler vs Hiper-parametreler

**Parametreler:**
* W<sup>[<i>1</i>]</sup>, W<sup>[<i>2</i>]</sup>, W<sup>[<i>3</i>]</sup>
* b<sup>[<i>1</i>]</sup>, b<sup>[<i>2</i>]</sup>
* ......


**Hiper-parametreler:**

* Ã–ÄŸrenme hÄ±zÄ±
* Ä°terasyon sayÄ±sÄ±
* Gizli katmanlarÄ±n sayÄ±sÄ±
* gizli birimlerin sayÄ±sÄ±
* Aktivasyon Fonksiyonunun SeÃ§imi 
* ......

> Hiperparametrelerin parametreleri kontrol ettiÄŸini sÃ¶yleyebiliriz ğŸ¤”
# ğŸ“š Yapay Sinir AÄŸlarÄ±nÄ±n KavramlarÄ±

Yapay Sinir AÄŸlarÄ±nÄ±n temel kavramlarÄ±

## ğŸ­ Temel Sinir AÄŸÄ±

<img src="../res/BasicANN.png" width="300"  />

> **Convention:** GiriÅŸ katmanÄ± sayÄ±lmadÄ±ÄŸÄ±ndan gÃ¶rÃ¼ntÃ¼deki NN 2 kat NN olarak adlandÄ±rÄ±lÄ±r ğŸ“¢â—

## ğŸ“š Terimler

| Terim            | AÃ§Ä±klama      |
| ---------------  |---------------|
| GiriÅŸ KatmanÄ±    | NN'nin giriÅŸlerini iÃ§eren katmandÄ±r |
| Gizli Katman     | HesaplamalÄ± iÅŸlemlerin yapÄ±ldÄ±ÄŸÄ± katman |
| Ã‡Ä±kÄ±ÅŸ KatmanÄ±    | NN'nin son katmanÄ± ve tahmin edilen deÄŸerin _yÌ‚_ Ã¼retilmesinden sorumludur |
| NÃ¶ron            | Bir matematik fonksiyonu iÃ§in bir yer tutucu, girdilere bir fonksiyon uygular ve Ã§Ä±ktÄ± saÄŸlar |
| Aktivasyon Fonksiyonu | DÃ¶nÃ¼ÅŸÃ¼mler uygulayarak bir dÃ¼ÄŸÃ¼mÃ¼n giriÅŸ sinyalini bir Ã§Ä±kÄ±ÅŸ sinyaline dÃ¶nÃ¼ÅŸtÃ¼ren bir fonksiyon |
| Shallow NN       | Az sayÄ±da gizli katmana sahip NN (bir veya iki)  |
| Deep NN          | Ã‡ok sayÄ±da gizli katmanÄ± olan NN |
| n<sup>[l]</sup>  | _l_ katmanÄ±ndaki nÃ¶ron sayÄ±sÄ± |


## ğŸ§  Yapay bir nÃ¶ron ne yapar?
It calculates a _weighted sum_ of its input, adds a bias and then decides whether it should be _fired_ or not due to an activaiton function
> My detailed notes on activaiton functions are [here](https://github.com/asmaamirkhan/DeepLearningNotes/tree/master/6-NNConcepts/3-ActivationFunctions.md) ğŸ‘©â€ğŸ«




## ğŸ‘©â€ğŸ”§ Parameters Dimension Control

| Parameter        | Dimension     |
| ---------------  |---------------|
| w<sup>[<i>l</i>]</sup>   |  (n<sup>[<i>l</i>]</sup>,n<sup>[<i>l-1</i>]</sup>) |
| b<sup>[<i>l</i>]</sup>   |  (n<sup>[<i>l</i>]</sup>,1) |
| dw<sup>[<i>l</i>]</sup>  |  (n<sup>[<i>l</i>]</sup>,n<sup>[<i>l-1</i>]</sup>) |
| db<sup>[<i>l</i>]</sup>  |  (n<sup>[<i>l</i>]</sup>,1) |


> Making sure that these dimensions are true help us to write better and bug-free :bug: codes

## ğŸˆ Summary of Forward Propagation Process

|                  |                 |
| ---------------- | --------------- |
| **Input:**       |  a<sup>[<i>l</i>-1]</sup> |
| **Output:**      |  a<sup>[<i>l</i>]</sup>, chache (z<sup>[<i>l</i>]</sup>) |

**Vectorized Equations:**

<img src="../res/formulas/ForwardProp.png" height="80"  />

## ğŸˆ Summary of Back Propagation Process

|                  |                 |
| ---------------- | --------------- |
| **Input:**       |  da<sup>[<i>l</i>]</sup> |
| **Output:**      | da<sup>[<i>l</i>-1]</sup>, dW<sup>[<i>l</i>]</sup>, db<sup>[<i>l</i>]</sup> |

**Vectorized Equations:**

<img src="../res/formulas/BackProp1.png" height="30"  />
<br>
<img src="../res/formulas/BackProp2.png" height="50"  />
<br>
<img src="../res/formulas/BackProp3.png" height="50"  />
<br>
<img src="../res/formulas/BackProp4.png" height="30"  />

## â°â° To Put Forward Prop. and Back Prop. Together

<img src="../res/ForBackSummary.png" width="500"  />

> ğŸ˜µğŸ¤•

## âœ¨ Parameters vs Hyperparameters

**Parameters:**
* W<sup>[<i>1</i>]</sup>, W<sup>[<i>2</i>]</sup>, W<sup>[<i>3</i>]</sup>
* b<sup>[<i>1</i>]</sup>, b<sup>[<i>2</i>]</sup>
* ......


**Hyperparameters:**

* Learning rate
* Number of iterations
* Number of hidden layers
* Number of hidden units
* Choice of activation function
* ......

> We can say that hyperparameters control parameters ğŸ¤”
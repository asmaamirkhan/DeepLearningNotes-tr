# ğŸ¨ Softmax Regression
**Lojistik regresyona benzeterek anlayabiliriz:** ğŸ˜‹

Lojistik regresyonun 0 ile 1,0 arasÄ±nda bir ondalÄ±klÄ± sayÄ± Ã¼rettiÄŸini hatÄ±rla, Ã–rneÄŸin, bir e-posta sÄ±nÄ±flandÄ±rÄ±cÄ±sÄ±ndan 0,8'lik bir lojistik regresyon Ã§Ä±ktÄ±sÄ±, bir e-postanÄ±n% 80'i spam olma olasÄ±lÄ±ÄŸÄ±nÄ± ve% 20'sinin spam olmadÄ±ÄŸÄ±nÄ± gÃ¶sterir. Yani, bir e-postanÄ±n spam olma ve spam olmama ihtimalinin toplamÄ± 1.0'dir.

Softmax bu fikri **Ã‡OKLU-SINIF** dÃ¼nyasÄ±na geniÅŸletiyor. Yani, Softmax, her sÄ±nÄ±fa Ã§ok sÄ±nÄ±flÄ± bir problemde ondalÄ±k olasÄ±lÄ±klar verir. **Bu olasÄ±lÄ±klarÄ±n toplamÄ± 1.0'e eÅŸittir**.

* Onun diÄŸer ismi _Maximum Entropy (MaxEnt) Classifier_

Softmax regresyonunun lojistik regresyonun genelleÅŸtirdiÄŸini sÃ¶yleyebiliriz.
> Lojistik regresyon, C = 2 olan softmax'Ä±n Ã¶zel bir durumudur ğŸ¤”

### ğŸ“š Notasyon
C = sÄ±nÄ±f sayÄ±sÄ± = Ã§Ä±kÄ±ÅŸ katmanÄ±nÄ±n birim sayÄ±sÄ±

Yani,  $$\hat{y}_j$$   (C, 1) boyutunda bir vektÃ¶rdÃ¼r.

### ğŸ¨ Softmax KatmanÄ±
Softmax, Ã§Ä±ktÄ± katmanÄ±ndan hemen Ã¶nce bir sinir aÄŸÄ± katmanÄ± vasÄ±tasÄ±yla uygulanÄ±r. Softmax katmanÄ±, Ã§Ä±kÄ±ÅŸ katmanÄ± ile aynÄ± sayÄ±da dÃ¼ÄŸÃ¼me sahip olmalÄ±dÄ±r.

<img src="../res/SoftmaxLayer.png" width="300"  />

### ğŸ’¥ Softmax Aktivasyon Fonksiyonu

$$Softmax(x_i)\frac{exp(x_i)}{\sum_{j}exp(x_j)}$$

### ğŸ”¨ Hard Max Fonksiyonu 
Softmax katmanÄ±nÄ±n Ã§Ä±ktÄ±sÄ±nÄ± alÄ±r ve  _1 vs 0 vector_ (adlandÄ±rdÄ±ÄŸÄ±ma gÃ¶re ğŸ¤­) vektÃ¶rÃ¼ne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r, o da bizim  _yÌ‚_'iz olacak

Ã–renÄŸin:
```
t = 0.13  ==> Ì‚y = 0
    0.75          1
    0.01          0
    0.11          0
```
Ve bunun gibi ğŸ¾

### ğŸ” KayÄ±p Fonksiyonu

$$L(\hat{y},y)=-\sum_{j=1}^{c}y_jlog(\hat{y}_j)$$

> Y ve  yÌ‚ (C,m) boyutunda matrislerdir ğŸ‘©â€ğŸ”§

## ğŸŒ YazÄ±nÄ±n AslÄ±
- [Burada ğŸ¾](https://dl.asmaamir.com/0-nnconcepts/7-multiclassclassification)

## ğŸ§ Daha Fazla Oku
* [Long story short from Google documentation](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax)
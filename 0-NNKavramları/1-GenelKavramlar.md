# ğŸ” Esas Problem
AÅŸaÄŸÄ±da verilen veri seti iÃ§in:  

$$[(x^{1},y^{1}), (x^{2},y^{2}), ...., (x^{m},y^{m})]$$

Bizim amacÄ±mÄ±z:

$$\hat{y}^{(i)} \approx y^{(i)}$$


## ğŸ“š Temel Kavramlar ve Notasyonlar

| Kavram          | AÃ§Ä±klama      |
| --------------- |---------------|
| `m`             | Veri setindeki Ã¶rnek sayÄ±sÄ±   |
| $$x^{(i)}$$     | Veri setindeki `i`'inci Ã¶rnek  |
| `Å·`             | Tahmin edilen Ã§Ä±ktÄ± |
| KayÄ±p Fonksiyonu _Loss Function_ `ğ“›(Å·, y)` | **Tek** bir Ã¶rnek iÃ§in hata hesaplama fonksiyonu |
| Cost Function _Maliyet Fonksiyonu_ `ğ™¹(w, b)` | TÃ¼m eÄŸitim setinin kayÄ±p fonksiyonlarÄ±nÄ±n ortalamasÄ±  |
| Konveks Fonksiyon | Tek bir yerel deÄŸere sahip bir fonksiyon |
| Konveks Olmayan Fonksiyon | Ã‡ok sayÄ±da farklÄ± yerel deÄŸere sahip bir fonksiyon |
| Gradyan Ä°niÅŸi _Gradient Descent_ | Maliyet Fonksiyonunun global deÄŸerini bulmak iÃ§in kullanÄ±lan iteratif bir optimizasyon yÃ¶ntemidir |

> BaÅŸka bir deyiÅŸle: Maliyet Fonksiyonu `w` ve `b` veri seti iÃ§in ne kadar iyi olduklarÄ±nÄ± Ã¶lÃ§er. Ona dayanarak, en iyi `w` ve `b` deÄŸerleri, `ğ™¹(w, b)`'Ä± mÃ¼mkÃ¼n olduÄŸunca kÃ¼Ã§Ã¼lten deÄŸerlerdir 

## ğŸ“‰ Gradyan Ä°niÅŸi 
Genel FormÃ¼l:

$$w:=w-\alpha\frac{dJ(w,b)}{dw}$$

$$b:=b-\alpha\frac{dJ(w,b)}{dw}$$


> `Î±` _(alpha)_ **Ã–ÄŸrenme HÄ±zÄ±**'dir (Learning Rate) 

## ğŸ¥½ Ã–ÄŸrenme HÄ±zÄ± (Learning Rate)
Model aÄŸÄ±rlÄ±klarÄ± her gÃ¼ncellendiÄŸinde karÅŸÄ±lÄ±k gelen tahmini hata nedeniyle her Gradyan Ä°niÅŸi tekrarÄ±nÄ±n adÄ±mÄ±nÄ±n boyutunu belirleyen pozitif bir skalardÄ±r, bu nedenle bir sinir aÄŸÄ± modelinin ne kadar hÄ±zlÄ± veya yavaÅŸ bir problemi Ã¶ÄŸrendiÄŸini kontrol eder.

### ğŸ€ Ä°yi Ã–ÄŸrenme HÄ±zÄ±

<img src="../res/GoodSGD.gif" width="300"  />

### ğŸ’¢ KÃ¶tÃ¼ Ã–ÄŸrenme HÄ±zÄ±

<img src="../res/BadSGD.gif" width="300"  />

## ğŸŒ YazÄ±nÄ±n AslÄ±
- [Burada ğŸ¾](https://dl.asmaamir.com/0-nnconcepts/1-generalconcepts)

## ğŸ§ Referanslar
* [Introduction to Artificial Neural Networks (ANN)](https://searchenterpriseai.techtarget.com/definition/neural-network)
* [More on Learning Rate](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)
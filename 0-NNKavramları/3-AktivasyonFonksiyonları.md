# ğŸ’¥ Yapay Sinir AÄŸlarÄ±nda Aktivasyon FonksiyonlarÄ± 
Aktivasyon FonksiyonlarÄ±nÄ±n temel amacÄ±, bir A-NN'deki bir dÃ¼ÄŸÃ¼mÃ¼n giriÅŸ sinyalini bir dÃ¶nÃ¼ÅŸÃ¼m uygulayarak bir Ã§Ä±kÄ±ÅŸ sinyaline dÃ¶nÃ¼ÅŸtÃ¼rmektir.
Bu Ã§Ä±kÄ±ÅŸ sinyali ÅŸimdi yÄ±ÄŸÄ±ndaki bir sonraki katmana girdi olarak kullanÄ±lÄ±r.

## ğŸ“ƒ Types of Activaiton Functions

| Fonksiyon                    | AÃ§Ä±klama                                                  |
| ---------------------------- | --------------------------------------------------------- |
| Lineer Aktivasyon Fonksiyonu | Verimsiz, regresyonda kullanÄ±lÄ±r                          |
| Sigmoid Fonksiyonu           | Ä°kili sÄ±nÄ±flandÄ±rma problemlerinde Ã§Ä±ktÄ± katmanÄ± iÃ§in iyi |
| Tanh Fonksiyonu              | Sigmoid fonksiyonundan daha iyidir                        |
| Relu Fonksiyonu âœ¨          |  Gizli katmanlar iÃ§in varsayÄ±lan seÃ§im                     |
| Leaky Relu Fonksiyonu        |  Relu'dan birazcÄ±k daha iyi, Relu daha popÃ¼ler            |


### ğŸ“ˆ Lineer Aktivasyon Fonksiyonu (Birim Fonksiyonu)

**FormÃ¼l:**

<img src="../res/formulas/LinearActivation.png" height="30"  />


**Grafik:**

<img src="../res/LinearActivationGraph.png" width="300"  />

> Regresyon problemlerindeki Ã§Ä±ktÄ± katmanÄ±nda kullanÄ±labilir. 

### ğŸ© Sigmoid Fonksiyonu

**FormÃ¼l:**

<img src="../res/formulas/Sigmoid.png" height="40"  />


**Grafik:**

<img src="../res/SigmoidGraph.png" width="300"  />


### ğŸ© Tangent Fonksiyonu

Neredeyse her zaman sigmoid fonksiyonundan Ã¼stÃ¼ndÃ¼r

**FormÃ¼l:**

<img src="../res/formulas/Tanh.png" height="40"  />

> Sigmoid fonksiyonunun kaydÄ±rÄ±lmÄ±ÅŸ versiyonu ğŸ¤”

**Grafik:**

<img src="../res/TanhGraph.PNG" width="300"  />

> Aktivasyon fonksiyonlarÄ± farklÄ± katmanlar iÃ§in farklÄ± olabilir, Ã¶rneÄŸin, gizli katman iÃ§in  _tanh_ fonksiyonunu, Ã§Ä±kÄ±ÅŸ katmanÄ± iÃ§in  _sigmoid_ fonksiyonunu kullanabiliriz 

### ğŸ™„ Tanh ve Sigmoid DezavantajlarÄ±
EÄŸer z Ã§ok bÃ¼yÃ¼kse veya Ã§ok kÃ¼Ã§Ã¼kse, bu fonksiyonun tÃ¼revi _(veya eÄŸimi)_ Ã§ok kÃ¼Ã§Ã¼k olur (0'a yakÄ±n olur) ve bu, gradient descent'i yavaÅŸlatabilir. ğŸ¢

### ğŸ© DoÄŸrultulmuÅŸ DoÄŸrusal Aktivasyon Ãœnitesi (Relu âœ¨) 
BaÅŸka ve Ã§ok popÃ¼ler bir seÃ§enek

**FormÃ¼l:**

<img src="../res/formulas/Relu.png" height="50"  />


**Grafik:**

<img src="../res/ReluGraph.png" width="300"  />


Yani tÃ¼rev, z pozitif olduÄŸunda 1 ve z negatif olduÄŸunda 0'dÄ±r.
> *Disadvantage:* z negatif iken tÃ¼rev=0'dÄ±r ğŸ˜

### ğŸ© SÄ±zdÄ±ran Relu

**FormÃ¼l:**

<img src="../res/formulas/LeakyRelu.png" height="50"  />


**Grafik:**

<img src="../res/LeakyReluGraph.png" width="300"  />


**Veya:** ğŸ˜›

<img src="../res/LeakyReluGraphMeme.png" width="150"  />


### ğŸ€ Relu'nun AvantajlarÄ±
* Z uzayÄ±nÄ±n Ã§oÄŸu, aktivasyon fonksiyonunun tÃ¼revi, 0'dan Ã§ok farklÄ±dÄ±r.
* NN tanh veya sigmoid kullanmaya gÃ¶re Ã§ok daha hÄ±zlÄ± Ã¶ÄŸrenecektir. 


## ğŸ¤” NN'lerin neden doÄŸrusal olmayan aktivasyon fonksiyonlara ihtiyacÄ± var?
Peki, eÄŸer lineer iÅŸlevi kullanÄ±rsak, NN sadece giriÅŸin lineer bir fonksiyonunu Ã§Ä±karÄ±r, yani NN'nin kaÃ§ katmanÄ± olursa olsun ğŸ™„, yaptÄ±ÄŸÄ± tek ÅŸey sadece lineer bir fonksiyonu hesaplamaktÄ±r ğŸ˜•

> â— Ä°ki doÄŸrusal fonksiyonun kompozisyonunun kendisinin doÄŸrusal bir fonksiyon olduÄŸunu unutma

## ğŸ‘©â€ğŸ« Aktivasyon Fonksiyonu SeÃ§mek Ä°Ã§in Kurallar
* EÄŸer Ã§Ä±kÄ±ÅŸ 0 veya 1 ise (ikili sÄ±nÄ±flandÄ±rma) â¡ *sigmoid* Ã§Ä±kÄ±ÅŸ katmanÄ± iÃ§in uygundur
* DiÄŸer bÃ¼tÃ¼n birimler iÃ§in â¡ *Relu* âœ¨ 
  
> Aktivasyon fonksiyonu iÃ§in relu'nun varsayÄ±lan seÃ§enek olduÄŸunu sÃ¶yleyebiliriz

Not:

> Bu fonksiyonlardan hangisinin en iyi sonucu verdiÄŸinden emin deÄŸil isen ğŸ˜µ, hepsini dene ğŸ¤• ve farklÄ± validasyon seti Ã¼zerinden deÄŸerlendir ve hangisinin daha iyi Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶r ve kullan ğŸ¤“ğŸ˜‡

## ğŸ§ Daha Fazla Oku
* [Which Activation Function Should I Use? (Siraj Raval :sparkles:)](https://www.youtube.com/watch?v=-7scQpJT7uo)
* [Activation Functions in Neural Networks](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)
* [Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)

# ğŸŒŒ RNN'deki Vanishing Gradients
- 10.000 zaman adÄ±mÄ± bÃ¼yÃ¼klÃ¼ÄŸÃ¼nde bir dizi verisini iÅŸleyen bir RNN, optimize edilmesi Ã§ok zor olan 10.000 derin katmana sahiptir ğŸ™„
- Derin Sinir AÄŸlarÄ±nda da, daha derin aÄŸlar _vanishing gradient_ problem sorununa giriyor ğŸ¥½ 
- Bu da, uzun dizi boyutuna sahip RNN'lerde de olur ğŸ›

## ğŸ§™â€â™€ï¸ Ã‡Ã¶zÃ¼mler
- **GRU** _Gated Recurrent Unit_ 
- **LSTM** _Long Short-Term Memory_ 

## ğŸšª Gated Recurrent Unit (GRU)
GRU'lar standart tekrarlayan sinir aÄŸlarÄ±nÄ±n geliÅŸmiÅŸ versiyonlarÄ±dÄ±r âœ¨, GRU _gÃ¼ncelleme kapÄ±sÄ± ve sÄ±fÄ±rlama kapÄ±sÄ±nÄ±_ kullanÄ±r 
- Temel olarak, bunlar Ã§Ä±ktÄ±ya hangi bilgilerin aktarÄ±lmasÄ± gerektiÄŸine karar veren iki vektÃ¶rdÃ¼r
- Onlarla ilgili Ã¶zel olan ÅŸey, bilgiyi uzun zaman Ã¶nce tutmak iÃ§in eÄŸitilebilecekleridir
  - Zamanla kaybolmadan veya tahminle ilgili bilgileri Ã§Ä±karmadan
  
| KapÄ±                 | AÃ§Ä±klama                                    |
| -------------------- |---------------------------------------------|
| ğŸ” GÃ¼ncelleme KapÄ±sÄ± | Modele, geÃ§miÅŸ bilgilerin ne kadarÄ±nÄ±n (Ã¶nceki zaman adÄ±mlarÄ±ndan) geleceÄŸe aktarÄ±lmasÄ± gerektiÄŸini belirlemede yardÄ±mcÄ± olur |
| 0ï¸âƒ£ SÄ±fÄ±rlama KapÄ±sÄ±  | Modele, geÃ§miÅŸ bilgilerin ne kadarÄ±nÄ±n **unutacaÄŸÄ±na** karar vermede yardÄ±mcÄ± olur |

### ğŸ” GÃ¼ncelleme KapÄ±sÄ±
Bu kapÄ± gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, _vanishing gradient_ problemi ortadan kalkar Ã§Ã¼nkÃ¼ model kendi baÅŸÄ±na geÃ§miÅŸ bilgilerin ne kadarÄ±nÄ± geleceÄŸe taÅŸÄ±dÄ±ÄŸÄ±nÄ± Ã¶ÄŸreniyor.
> KÄ±saca: Åimdi ne kadar geÃ§miÅŸ Ã¶nemli olmalÄ±? ğŸ™„

### 0ï¸âƒ£ SÄ±fÄ±rlama KapÄ±sÄ±
GeÃ§miÅŸ bilgilerinin ne kadarÄ±nÄ±n unutulacaÄŸÄ±na karar vermek iÃ§in model tarafÄ±ndan kullanÄ±ldÄ±ÄŸÄ±ndan, bu kapÄ± gÃ¼ncelleme kapÄ±sÄ±yla karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda tam tersi bir iÅŸlevselliÄŸe sahiptir.
> KÄ±saca: Ã–nceki bilgi dÃ¼ÅŸÃ¼rÃ¼lecek mi? ğŸ™„

### ğŸ’¬ Current Memory Content
GeÃ§miÅŸten ilgili bilgileri saklamak iÃ§in sÄ±fÄ±rlama kapÄ±sÄ±nÄ± kullanacak bellek iÃ§eriÄŸi.

### ğŸˆ Final Memory at Current Time Step
Mevcut birim iÃ§in bilgi tutan ve onu daha sonra aÄŸa iletecek bir vektÃ¶r.

### ğŸ‘€ GÃ¶rselleÅŸtirme

<img src="../res/GRU.png" width="600"  />

### ğŸ‰ GRU Ã–zeti
- **vanishing gradient** problemni gidermek iÃ§in bir Ã§Ã¶zÃ¼mdÃ¼r 
- Model her seferinde yeni giriÅŸi kaybetmekte kalmÄ±yor, ilgili bilgileri saklÄ±yor ve aÄŸÄ±n bir sonraki zaman adÄ±mlarÄ±na aktarÄ±yor

## ğŸ¤¸â€â™€ï¸ Long Short-Term Memory

### 0ï¸âƒ£ Forget Gate
- Let's assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. 
- If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. 
- In an LSTM, the forget gate let's us do this:

$$\Gamma ^{<t>}_f = \sigma(W_f[a^{<t-1>}, x^{<t>}]+b_f)$$

- Here,  $W_f$  are weights that govern the forget gate's behavior. We concatenate  $$[a^{<t-1>}, x^{<t>}]$$  and multiply by  $$W_f$$. The equation above results in a vector  $$\Gamma_f^{<t>}$$  with values between 0 and 1. 
- This forget gate vector will be multiplied element-wise by the previous cell state $$c^{<t-1>}$$. 
- So if one of the values of $$\Gamma_f^{<t>}$$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of  $$c^{<t-1>}$$ . 
- If one of the values is 1, then it will keep the information.

### ğŸ”„ Update Gate
Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formula for the update gate:

$$\Gamma ^{<t>}_u = \sigma(W_u[a^{<t-1>}, x^{<t>}]+b_u)$$

Similar to the forget gate, here  $$\Gamma_u^{<t>}$$  is again a vector of values between 0 and 1. This will be multiplied element-wise with  $$\tilde{c}^{<t>}$$, in order to compute $$c^{âŸ¨tâŸ©}$$.

### ğŸ‘©â€ğŸ”§ Updating the Cell
To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is:

$$\tilde{c}^{<t>}=tanh(W_c[a^{<t-1>}, x^{<t>}]+b_c)$$

Finally, the new cell state is:

$$c^{<t>}=\Gamma _f^{<t>}*c^{<t-1>} + \Gamma _u^{<t>}*\tilde{c}^{<t>}$$

### ğŸšª Output Gate
To decide which outputs we will use, we will use the following two formulas:

$$\Gamma _o^{<t>}=\sigma(W_o[a^{<t-1>}, x^{<t>}]+b_o)$$

$$a^{<t>} = \Gamma _o^{<t>}*tanh(c^{<t>})$$

Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the _tanh_ of the previous state.

<img src="../res/RNNLSTM.png" width="600"  />

> GRU is newer than LSTM, LSTM is more powerful but GRU is easier to implement ğŸš§

## ğŸ§ Read More
- [What are RNNs and GRUs](https://towardsdatascience.com/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69)
- [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
- [Detailed LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
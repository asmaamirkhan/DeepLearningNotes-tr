# ğŸ”„ Tekrarlayan Sinir AÄŸlarÄ± (RNN)

## ğŸ” TanÄ±m
Ã–nceki Ã§Ä±kÄ±ÅŸlarÄ±n sonraki katmanlara giriÅŸ olarak kullanÄ±lmasÄ±na izin veren bir sinir aÄŸÄ± sÄ±nÄ±fÄ±dÄ±r
> EÄŸitim sÄ±rasÄ±nda Ã¶ÄŸrendikleri ÅŸeyleri hatÄ±rlarlar âœ¨

## ğŸ§± Mimari

### ğŸ”¶ RNN TamamÄ±nÄ±n Mimarisi
<img src="../res/RNNStructure2.png" width="600"  />

<img src="../res/RNNStructure.png" width="600"  />

### ğŸ§© Bir RNN HÃ¼cresi

<img src="../res/RNNCell.png" width="600"  />

Temel bir RNN hÃ¼cresi. $$x^{âŸ¨tâŸ©}$$'yi (Åimdiki girdi) ve $$a^{âŸ¨tâˆ’1âŸ©}$$'yi (geÃ§miÅŸten gelen bilgileri iÃ§eren Ã¶nceki gizli durum) girdi olarak alÄ±r, bir sonraki RNN hÃ¼cresine verilen ve aynÄ± zamanda $$y^{âŸ¨tâŸ©}$$'yi tahmin etmek iÃ§in kullanÄ±lan $$a^{âŸ¨tâŸ©}$$'yi Ã§Ä±ktÄ± olarak verir 

## â© Forward Propagation
**$$a^{<t>}$$'yi hesaplamak iÃ§in:**

$$a^{<t>}=g(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)$$

**$$\hat{y}^{<t>}$$'yi hesaplamak iÃ§in:**

$$\hat{y}^{<t>} = g(W_{ya}a^{<t>}+b_y)$$

### ğŸ‘€ GÃ¶rselleÅŸtirme

<img src="../res/RNNForwardVis.png" width="600"  />

## âª Back Propagation
**KayÄ±p fonksiyonu aÅŸaÄŸÄ±daki ÅŸekilde tanÄ±mlanÄ±r**

$$L^{<t>}(\hat{y}^{<t>}, y^{<t>})=-y^{<t>}log(\hat{y})-(1-y^{<t>})log(1-\hat{y}^{<t>})$$

$$L(\hat{y},y)=\sum_{t=1}^{T_y}L^{<t>}(\hat{y}^{<t>}, y^{<t>})$$

## ğŸ¨ RNN TÃ¼rleri
- 1ï¸âƒ£ â¡ 1ï¸âƒ£ **One-to-One** (Klasik ANN)
- 1ï¸âƒ£ â¡ ğŸ”¢ **One-to-Many** (MÃ¼zik Ãœretme)
- ğŸ”¢ â¡ 1ï¸âƒ£ **Many-to-One** (Anlamsal Analiz)
- ğŸ”¢ â¡ ğŸ”¢ **Many-to-Many** $$T_x = T_y$$ (KonuÅŸma tanÄ±ma)
- ğŸ”¢ â¡ ğŸ”¢ **Many-to-Many** $$T_x \neq T_y$$ (Makine Ã‡evirisi)

<img src="../res/RNNTypes.png" width="600"  />

# ğŸ”¥ GeliÅŸmiÅŸ Tekrarlayan Sinir AÄŸlarÄ±

## ğŸ”„ Ã‡ift YÃ¶nlÃ¼ Tekrarlayan Yapay Sinir AÄŸlarÄ± (BRNN)
- BirÃ§ok uygulamada, tÃ¼m giriÅŸ sÄ±rasÄ±na baÄŸlÄ± olabilecek bir $$y^{(t)}$$ tahmini Ã§Ä±karmak isteyebiliriz
- Bidirectional RNN'ler dizinin baÅŸlangÄ±cÄ±ndan baÅŸlayarak zaman iÃ§inde **ileri** hareket eden bir RNN'i, dizinin sonundan baÅŸlayarak zaman boyunca **geri** hareket eden bir RNN ile birleÅŸtirirler.

### ğŸ’¬ BaÅŸka Bir DeyiÅŸle
- Ã‡ift YÃ¶nlÃ¼ Tekrarlayan Yapay Sinir AÄŸlarÄ± aslÄ±nda iki baÄŸÄ±msÄ±z RNN bir araya getiriyorlar. 
- GiriÅŸ dizisi bir aÄŸ iÃ§in normal zaman sÄ±rasÄ±yla, diÄŸeri iÃ§in ters zaman sÄ±rasÄ±yla aktarÄ±lÄ±r. 
- Ä°ki aÄŸÄ±n Ã§Ä±kÄ±ÅŸlarÄ± genellikle her zaman adÄ±mÄ±nda birleÅŸtirilir.
- ğŸ‰ Bu yapÄ±, aÄŸlarÄ±n, her adÄ±mda dizi hakkÄ±nda hem geri hem de ileri bilgiye sahip olmalarÄ±nÄ± saÄŸlar. 

### ğŸ‘ Dezavantaj
Tahmin yapmadan Ã¶nce tÃ¼m veri dizisine ihtiyacÄ±mÄ±z vardÄ±r.

>e.g: gerÃ§ek zamanlÄ± konuÅŸma tanÄ±ma iÃ§in uygun deÄŸildir 

### ğŸ‘€ GÃ¶rselleÅŸtirme

<img src="../res/BRNN.png" width="600"  />


## ğŸ•¸ Derin RNN'ler
Ã‡oÄŸu RNN hesaplamalarÄ±, Ã¼Ã§ parametre bloÄŸuna ve iliÅŸkili dÃ¶nÃ¼ÅŸÃ¼mlere ayrÄ±ÅŸtÄ±rÄ±labilir.:
1. GiriÅŸten gizli duruma, $$x^{(t)}$$ â¡ $$a^{(t)}$$
2. Ã–nceki gizli durumdan sonraki gizli duruma, $$a^{(t-1)}$$ â¡ $$a^{(t)}$$
3. Gizli durumdan Ã§Ä±ktÄ±ya, $$a^{(t)}$$ â¡ $$y^{(t)}$$

Derin tekrarlayan aÄŸlarla sonuÃ§lanan yukarÄ±daki dÃ¶nÃ¼ÅŸÃ¼mlerin her biri iÃ§in birden Ã§ok katman kullanabiliriz ğŸ˜‹

### ğŸ‘€ GÃ¶rselleÅŸtirme

<img src="../res/DeepRNN.PNG" width="600"  />

## âŒ Problem: Vanishing Gradients with RNNs
- 10.000 zaman adÄ±mÄ± bÃ¼yÃ¼klÃ¼ÄŸÃ¼nde bir dizi verisini iÅŸleyen bir RNN, optimize edilmesi Ã§ok zor olan 10.000 derin katmana sahiptir ğŸ™„
- Derin Sinir AÄŸlarÄ±nda da, daha derin aÄŸlar _vanishing gradient_ problem sorununa giriyor ğŸ¥½ 
- Bu da, uzun dizi boyutuna sahip RNN'lerde de olur ğŸ›

### ğŸ§™â€â™€ï¸ Ã‡Ã¶zÃ¼mler
- RNN'deki Vanishing Gradients problemi Ã¼zerine [YazÄ±mÄ±](./2-VanishingGradients.md) okuyabilirsin  ğŸ¤¸â€â™€ï¸

## ğŸŒ YazÄ±nÄ±n AslÄ±
- [Burada ğŸ¾](https://dl.asmaamir.com/9-sequencemodels/1-rnns)

## ğŸ§ Daha Fazla Oku
- [Recurrent Neural Networks Cheatsheet âœ¨](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#)
- [All About RNNs ğŸš€](https://medium.com/@jianqiangma/all-about-recurrent-neural-networks-9e5ae2936f6e)
# ğŸŒŒ RNN'deki Vanishing Gradients
- 10.000 zaman adÄ±mÄ± bÃ¼yÃ¼klÃ¼ÄŸÃ¼nde bir dizi verisini iÅŸleyen bir RNN, optimize edilmesi Ã§ok zor olan 10.000 derin katmana sahiptir ğŸ™„
- Derin Sinir AÄŸlarÄ±nda da, daha derin aÄŸlar _vanishing gradient_ problem sorununa giriyor ğŸ¥½ 
- Bu da, uzun dizi boyutuna sahip RNN'lerde de olur ğŸ›

## ğŸ§™â€â™€ï¸ Ã‡Ã¶zÃ¼mler
- **GRU** _Gated Recurrent Unit_ 
- **LSTM** _Long Short-Term Memory_ 

## ğŸšª Gated Recurrent Unit (GRU)
GRU'lar standart tekrarlayan sinir aÄŸlarÄ±nÄ±n geliÅŸmiÅŸ versiyonlarÄ±dÄ±r âœ¨, GRU _gÃ¼ncelleme kapÄ±sÄ± ve sÄ±fÄ±rlama kapÄ±sÄ±nÄ±_ kullanÄ±r 
- Temel olarak, bunlar Ã§Ä±ktÄ±ya hangi bilgilerin aktarÄ±lmasÄ± gerektiÄŸine karar veren iki vektÃ¶rdÃ¼r
- Onlarla ilgili Ã¶zel olan ÅŸey, bilgiyi uzun zaman Ã¶nce tutmak iÃ§in eÄŸitilebilecekleridir
  - Zamanla kaybolmadan veya tahminle ilgili bilgileri Ã§Ä±karmadan
  
| KapÄ±                 | AÃ§Ä±klama                                    |
| -------------------- |---------------------------------------------|
| ğŸ” GÃ¼ncelleme KapÄ±sÄ± | Modele, geÃ§miÅŸ bilgilerin ne kadarÄ±nÄ±n (Ã¶nceki zaman adÄ±mlarÄ±ndan) geleceÄŸe aktarÄ±lmasÄ± gerektiÄŸini belirlemede yardÄ±mcÄ± olur |
| 0ï¸âƒ£ SÄ±fÄ±rlama KapÄ±sÄ±  | Modele, geÃ§miÅŸ bilgilerin ne kadarÄ±nÄ±n **unutacaÄŸÄ±na** karar vermede yardÄ±mcÄ± olur |

### ğŸ” GÃ¼ncelleme KapÄ±sÄ±
Bu kapÄ± gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, _vanishing gradient_ problemi ortadan kalkar Ã§Ã¼nkÃ¼ model kendi baÅŸÄ±na geÃ§miÅŸ bilgilerin ne kadarÄ±nÄ± geleceÄŸe taÅŸÄ±dÄ±ÄŸÄ±nÄ± Ã¶ÄŸreniyor.
> KÄ±saca: Åimdi ne kadar geÃ§miÅŸ Ã¶nemli olmalÄ±? ğŸ™„

### 0ï¸âƒ£ SÄ±fÄ±rlama KapÄ±sÄ±
GeÃ§miÅŸ bilgilerinin ne kadarÄ±nÄ±n unutulacaÄŸÄ±na karar vermek iÃ§in model tarafÄ±ndan kullanÄ±ldÄ±ÄŸÄ±ndan, bu kapÄ± gÃ¼ncelleme kapÄ±sÄ±yla karÅŸÄ±laÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda tam tersi bir iÅŸlevselliÄŸe sahiptir.
> KÄ±saca: Ã–nceki bilgi dÃ¼ÅŸÃ¼rÃ¼lecek mi? ğŸ™„

### ğŸ’¬ Current Memory Content
GeÃ§miÅŸten ilgili bilgileri saklamak iÃ§in sÄ±fÄ±rlama kapÄ±sÄ±nÄ± kullanacak bellek iÃ§eriÄŸi.

### ğŸˆ Final Memory at Current Time Step
Mevcut birim iÃ§in bilgi tutan ve onu daha sonra aÄŸa iletecek bir vektÃ¶r.

### ğŸ‘€ GÃ¶rselleÅŸtirme

<img src="../res/GRU.png" width="600"  />

### ğŸ‰ GRU Ã–zeti
- **vanishing gradient** problemini gidermek iÃ§in bir Ã§Ã¶zÃ¼mdÃ¼r 
- Model her seferinde yeni giriÅŸi kaybetmekte kalmÄ±yor, ilgili bilgileri saklÄ±yor ve aÄŸÄ±n bir sonraki zaman adÄ±mlarÄ±na aktarÄ±yor

## ğŸ¤¸â€â™€ï¸ Long Short-Term Memory (LSTM)

### 0ï¸âƒ£ SÄ±fÄ±rlama KapÄ±sÄ± (Forget Gate)
- Bir metinde kelimeler okuduÄŸumuzu varsayalÄ±m ve Ã¶rneÄŸin tekil mi yoksa Ã§oÄŸul mu olduÄŸu gibi gramer yapÄ±larÄ±nÄ± takip etmek iÃ§in bir LSTM kullanmak istiyoruz. 
- EÄŸer konu tekil bir kelimeden Ã§oÄŸul kelimeye geÃ§erse, tekil / Ã§oÄŸul durumun Ã¶nceden depolanmÄ±ÅŸ hafÄ±za deÄŸerinden kurtulmanÄ±n bir yolunu bulmalÄ±yÄ±z. 
- LSTM'de, sÄ±fÄ±rlama _(forget)_ kapÄ±sÄ± bunu yapmamÄ±za izin veriyor

$$\Gamma ^{<t>}_f = \sigma(W_f[a^{<t-1>}, x^{<t>}]+b_f)$$

- Burada,  $W_f$  sÄ±fÄ±rlama kapÄ±sÄ± davranÄ±ÅŸÄ±nÄ± yÃ¶neten aÄŸÄ±rlÄ±klardÄ±r. $$[a^{<t-1>} ve x^{<t>}]$$'yi birleÅŸtiriyoruz ve $$W_f$$ ile Ã§arpÄ±yoruz. YukarÄ±daki denklem, 0 ile 1 arasÄ±nda deÄŸerleri olan bir $$\Gamma_f^{<t>}$$ vektÃ¶rÃ¼yle sonuÃ§lanÄ±r 
- Bu sÄ±fÄ±rlama kapÄ±sÄ± vektÃ¶rÃ¼, Ã¶nceki hÃ¼cre durumu $$c^{<t-1>}$$ olan ile element-wise Ã§arpÄ±lÄ±r 
- EÄŸer $$\Gamma_f^{<t>}$$'nÄ±n deÄŸerlerinden biri 0 ise (veya 0'a yakÄ±nsa), LSTM'nin bu bilgi parÃ§asÄ±nÄ± $$c^{<t-1>}$$'nin karÅŸÄ±lÄ±k gelen bileÅŸeninden Ã§Ä±karmasÄ± gerektiÄŸi anlamÄ±na gelir (Ã¶rneÄŸin: tekil nesne).
- DeÄŸerlerden biri 1 ise, bilgiyi olduÄŸu gibi korunacaktÄ±r.

### ğŸ”„ GÃ¼ncelleme KapÄ±sÄ± (Update Gate)
OdaklandÄ±ÄŸÄ±mÄ±z nesnenin tekil olduÄŸunu unuttuÄŸumuzda, yeni nesnenin artÄ±k Ã§oÄŸul olduÄŸunu yansÄ±tacak ÅŸekilde gÃ¼ncellemenin bir yolunu bulmalÄ±yÄ±z. GÃ¼ncelleme kapÄ±sÄ±nÄ±n formÃ¼lÃ¼ aÅŸaÄŸÄ±daki gibidir:

$$\Gamma ^{<t>}_u = \sigma(W_u[a^{<t-1>}, x^{<t>}]+b_u)$$

SÄ±fÄ±rlama kapÄ±sÄ±nda olduÄŸuna benzer ÅŸekilde, burada $$\ Gamma_u^{<t>}$$ yine 0 ile 1 arasÄ±ndaki deÄŸerlerden oluÅŸan bir vektÃ¶rdÃ¼r. Bu, $$c^{âŸ¨tâŸ©}$$ 'i hesaplamak iÃ§in, $$\tilde{c}^{<t>}$$ ile _element-wise_ Ã§arpÄ±lacaktÄ±r.

### ğŸ‘©â€ğŸ”§ HÃ¼creyi GÃ¼ncelleme
Yeni nesneyi gÃ¼ncellemek iÃ§in Ã¶nceki hÃ¼cre durumumuza ekleyebileceÄŸimiz yeni bir sayÄ± vektÃ¶rÃ¼ oluÅŸturmamÄ±z gerekiyor. KullandÄ±ÄŸÄ±mÄ±z denklem aÅŸaÄŸÄ±daki gibidir:

$$\tilde{c}^{<t>}=tanh(W_c[a^{<t-1>}, x^{<t>}]+b_c)$$

Son olarak, yeni hÃ¼cre durumu:

$$c^{<t>}=\Gamma _f^{<t>}*c^{<t-1>} + \Gamma _u^{<t>}*\tilde{c}^{<t>}$$

### ğŸšª Ã‡Ä±kÄ±ÅŸ KapÄ±sÄ± (Output Gate)
Hangi Ã§Ä±ktÄ±larÄ± kullanacaÄŸÄ±mÄ±za karar vermek iÃ§in aÅŸaÄŸÄ±daki iki formÃ¼lÃ¼ kullanÄ±yoruz:

$$\Gamma _o^{<t>}=\sigma(W_o[a^{<t-1>}, x^{<t>}]+b_o)$$

$$a^{<t>} = \Gamma _o^{<t>}*tanh(c^{<t>})$$

Birinci denklemde, _sigmoid_ fonksiyonunu kullanarak neyin Ã§Ä±ktÄ±sÄ± alÄ±nacaÄŸÄ±na karar verirken, ikinci denklemde Ã¶nceki durumu _tanh_ fonksiyonu ile Ã§arpÄ±yoruz.

<img src="../res/RNNLSTM.png" width="600"  />

> GRU, LSTM'den daha yeni, LSTM daha gÃ¼Ã§lÃ¼, ancak GRU'nun uygulanmasÄ± daha kolay ğŸš§

## ğŸŒ YazÄ±nÄ±n AslÄ±
- [Burada ğŸ¾](https://dl.asmaamir.com/9-sequencemodels/2-vanishinggradients)

## ğŸ§ Daha Fazla Oku
- [What are RNNs and GRUs](https://towardsdatascience.com/what-is-a-recurrent-nns-and-gated-recurrent-unit-grus-ea71d2a05a69)
- [Understanding GRU Networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
- [Detailed LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)